{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKRWjlR77eik"
      },
      "source": [
        "# **Assignment 3 - Implementing object detectors; YOLO and Faster R-CNN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03XxIr9IZ40L"
      },
      "source": [
        "안녕하세요 AIKU 학회원 여러분, 이번 D2D의 세 번째 과제는 object detection입니다.\n",
        "\n",
        "여러분은 object detection을 위한 single-stage detector와 two-stage detector를 각각 하나씩 구현할 예정입니다.\n",
        "\n",
        "두 detection 방식의 가장 큰 차이는 single-stage detector는 region proposal과 classification을 동시에 진행하는 반면 two-stage는 이를 순차적으로 별개로 진행합니다.\n",
        "\n",
        "먼저, 대표적인 single-stage detector 모델 중 하나인 **YOLO** ([v1](https://arxiv.org/pdf/1506.02640.pdf) and [v2](https://arxiv.org/pdf/1612.08242.pdf))를 기반으로 하는 single-stage detector를 구현하고, 그 이후 Object Detection 강의에서 배웠던 two-stage detector 모델 중 하나인 [**Faster R-CNN**](https://arxiv.org/pdf/1506.01497.pdf)과 유사한 two-stage detector를 구현할 것입니다.\n",
        "\n",
        "특히, Faster R-CNN은 기존 Fast R-CNN 모델에 Region Proposal Network (RPN)을 추가한 형태입니다.\n",
        "\n",
        "이를 구현하여 가장 대표적인 Object Detection 데이터셋 중 하나인 PASCAL Visual Object Classes Challenge 2007 (PASCAL VOC 2007) 이미지들을 활용해 직접 object detection을 수행하고 mAP를 통해 성능 평가를 해보는 것이 목표입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfBk3NtRgqaV"
      },
      "source": [
        "# 1. Single-stage Object Detector; YOLO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLzv2QCubAS1"
      },
      "source": [
        "먼저, 여러분이 앞으로 detector를 구현하기 위해 필요한 여러 가지 기본적인 함수들을 정의해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubB_0e-UAOVK"
      },
      "source": [
        "## Install starter code\n",
        "\n",
        "과제 코드를 위한 유용한 함수를 import할 수 있는 라이브러리를 설치합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASkY27ZtA7Is"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/deepvision-class/starter-code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzqbYcKdz6ew"
      },
      "source": [
        "## Setup code\n",
        "기본적인 과제를 위한 setup을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzRdJ3uhe1CR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import coutils\n",
        "from coutils import extract_drive_file_id, register_colab_notebooks, \\\n",
        "                    fix_random_seed, rel_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "import copy\n",
        "import time\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# for plotting\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# data type and device for torch.tensor\n",
        "to_float = {'dtype': torch.float, 'device': 'cpu'}\n",
        "to_float_cuda = {'dtype': torch.float, 'device': 'cuda'}\n",
        "to_double = {'dtype': torch.double, 'device': 'cpu'}\n",
        "to_double_cuda = {'dtype': torch.double, 'device': 'cuda'}\n",
        "to_long = {'dtype': torch.long, 'device': 'cpu'}\n",
        "to_long_cuda = {'dtype': torch.long, 'device': 'cuda'}\n",
        "\n",
        "# for mAP evaluation\n",
        "!rm -rf mAP\n",
        "!git clone https://github.com/Cartucho/mAP.git\n",
        "!rm -rf mAP/input/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvUDZWGU3VLV"
      },
      "source": [
        "GPU 사용이 가능한지 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrAX9FOLpr9k"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available:\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjJ3uyYBg3Lw"
      },
      "source": [
        "## Load PASCAL VOC 2007 data\n",
        "\n",
        "아래 함수는 PASCAL VOC 2007 데이터셋을 다운로드 하여 PyTorch Dataset object를 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmD9Qrs2g7fI"
      },
      "outputs": [],
      "source": [
        "def get_pascal_voc2007_data(image_root, split='train'):\n",
        "  \"\"\"\n",
        "  Use torchvision.datasets\n",
        "  https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.VOCDetection\n",
        "  \"\"\"\n",
        "  from torchvision import datasets\n",
        "\n",
        "  train_dataset = datasets.VOCDetection(image_root, year='2007', image_set=split,\n",
        "                                    download=True)\n",
        "\n",
        "  return train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXc_Hw3JhVxw"
      },
      "source": [
        "아래 코드를 실행하면 PASCAL VOC 2007 training and validation datasets을 다운로드할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmEP5KQJzk0d"
      },
      "outputs": [],
      "source": [
        "# uncomment below to use the mirror link if the original link is broken\n",
        "# !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
        "train_dataset = get_pascal_voc2007_data('/content', 'train')\n",
        "val_dataset = get_pascal_voc2007_data('/content', 'val')\n",
        "\n",
        "# an example on the raw annotation\n",
        "import json\n",
        "print(json.dumps(train_dataset[1][1]['annotation'], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5MjBX9bkBtA"
      },
      "source": [
        "이러한 annotation을 사용하여 모델을 훈련하려면 이 dictionary data를 PyTorch 텐서로 변환해야 합니다.\n",
        "\n",
        "또한, 이미지를 Pytorch 텐서로 변환하고 224x224로 크기를 조정하는 전처리를 해야 합니다. 이 과제에서는 계산 효율성을 위해 낮은 해상도를 사용합니다.\n",
        "\n",
        "또한, 미니배치를 사용하여 모델을 학습시키고자 하므로 여러 이미지를 미니배치로 그룹화해야 합니다.\n",
        "\n",
        "이 두 가지 기능은 다음과 같은 함수를 통해 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfwTGpZn1L5U"
      },
      "outputs": [],
      "source": [
        "def pascal_voc2007_loader(dataset, batch_size, num_workers=0):\n",
        "  \"\"\"\n",
        "  Data loader for Pascal VOC 2007.\n",
        "  https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
        "  \"\"\"\n",
        "  from torch.utils.data import DataLoader\n",
        "  # turn off shuffle so we can index the original image\n",
        "  train_loader = DataLoader(dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=False, pin_memory=True,\n",
        "                            num_workers=num_workers,\n",
        "                            collate_fn=voc_collate_fn)\n",
        "  return train_loader\n",
        "\n",
        "\n",
        "class_to_idx = {'aeroplane':0, 'bicycle':1, 'bird':2, 'boat':3, 'bottle':4,\n",
        "                'bus':5, 'car':6, 'cat':7, 'chair':8, 'cow':9, 'diningtable':10,\n",
        "                'dog':11, 'horse':12, 'motorbike':13, 'person':14, 'pottedplant':15,\n",
        "                'sheep':16, 'sofa':17, 'train':18, 'tvmonitor':19\n",
        "}\n",
        "idx_to_class = {i:c for c, i in class_to_idx.items()}\n",
        "\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "def voc_collate_fn(batch_lst, reshape_size=224):\n",
        "    preprocess = transforms.Compose([\n",
        "      transforms.Resize((reshape_size, reshape_size)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "      ])\n",
        "\n",
        "    batch_size = len(batch_lst)\n",
        "\n",
        "    img_batch = torch.zeros(batch_size, 3, reshape_size, reshape_size)\n",
        "\n",
        "    max_num_box = max(len(batch_lst[i][1]['annotation']['object']) \\\n",
        "                      for i in range(batch_size))\n",
        "\n",
        "    box_batch = torch.Tensor(batch_size, max_num_box, 5).fill_(-1.)\n",
        "    w_list = []\n",
        "    h_list = []\n",
        "    img_id_list = []\n",
        "\n",
        "    for i in range(batch_size):\n",
        "      img, ann = batch_lst[i]\n",
        "      w_list.append(img.size[0]) # image width\n",
        "      h_list.append(img.size[1]) # image height\n",
        "      img_id_list.append(ann['annotation']['filename'])\n",
        "      img_batch[i] = preprocess(img)\n",
        "      all_bbox = ann['annotation']['object']\n",
        "      if type(all_bbox) == dict: # inconsistency in the annotation file\n",
        "        all_bbox = [all_bbox]\n",
        "      for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "        bbox = one_bbox['bndbox']\n",
        "        obj_cls = one_bbox['name']\n",
        "        box_batch[i][bbox_idx] = torch.Tensor([float(bbox['xmin']), float(bbox['ymin']),\n",
        "          float(bbox['xmax']), float(bbox['ymax']), class_to_idx[obj_cls]])\n",
        "\n",
        "    h_batch = torch.tensor(h_list)\n",
        "    w_batch = torch.tensor(w_list)\n",
        "\n",
        "    return img_batch, box_batch, w_batch, h_batch, img_id_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ad8hHvAlGdA"
      },
      "source": [
        "연산량을 위해 전체 데이터셋의 일부만 가져와서 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL-7Em_A1kdS"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(0, 2500)) # use 2500 samples for training\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 10)\n",
        "val_loader = pascal_voc2007_loader(val_dataset, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTyRHqwlC1Au"
      },
      "source": [
        "`DataLoader` object는 데이터의 배치를 리턴합니다.\n",
        "\n",
        "`DataLoader`의 first output은 `(B, 3, 224, 224)` size의  Tensor `img`입니다.\n",
        "\n",
        "`DataLoader`의 second output은 Tensor `ann` (shape `(B, N, 5)`)으로, batch에 있는 모든 이미지들의 모든 object에 대한 정보를 담고 있습니다.\n",
        "\n",
        "`img[i]`의 `j`th object에 대한 정보는 `ann[i, j] = (x_tl, y_tl, x_br, y_br, class)` 으로 표현되며, box의 top-left corner가 `(x_tl, y_tl)`, bottom-right corner가 `(x_br, y_br)`입니다. 이 좌표는 image가 224x224로 resize되기 전의 좌표입니다.\n",
        "\n",
        "`class`는 해당 bounding box에 대한 category label을 나타내는 정수입니다.\n",
        "\n",
        "`img[i]`가 $N_i$개의 object를 가지고 있다면, $N=\\max_i N_i$는 batch마다 다를 것입니다. $N$개보다 object 개수가 작은 image의 경우, 첫 $N_i$개의 행만 정보를 담고 있고, 나머지는 -1로 채워져있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZVYFJD32I_l"
      },
      "outputs": [],
      "source": [
        "img, ann, _, _, _ = next(iter(train_loader))\n",
        "\n",
        "print('img has shape: ', img.shape)\n",
        "print('ann has shape: ', ann.shape)\n",
        "\n",
        "print('Image 1 has only two annotated objects, so ann[1] is padded with -1:')\n",
        "print(ann[1])\n",
        "\n",
        "print('\\nImage 2 has six annotated objects:, so ann[2] is not padded:')\n",
        "print(ann[2])\n",
        "\n",
        "print('\\nEach row in the annotation tensor indicates (x_tl, y_tl, x_br, y_br, class).')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqISg-cs6vKM"
      },
      "source": [
        "## Coordinate transformation\n",
        "**이번 과제에서 우리는 CNN activation map (7x7)로 정의된 좌표 시스템을 사용할 예정입니다. top-left corner가 (0, 0), bottom-right corner가 (7, 7)입니다.**\n",
        "\n",
        "아래 함수를 통해 원래 이미지의 좌표 시스템에서 위에서 서술한 activation map 좌표 시스템으로 transformation을 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggnqmAXh6vJv"
      },
      "outputs": [],
      "source": [
        "def coord_trans(bbox, w_pixel, h_pixel, w_amap=7, h_amap=7, mode='a2p'):\n",
        "  \"\"\"\n",
        "  Coordinate transformation function. It converts the box coordinate from\n",
        "  the image coordinate system to the activation map coordinate system and vice versa.\n",
        "  In our case, the input image will have a few hundred of pixels in\n",
        "  width/height while the activation map is of size 7x7.\n",
        "\n",
        "  Input:\n",
        "  - bbox: Could be either bbox, anchor, or proposal, of shape Bx*x4\n",
        "  - w_pixel: Number of pixels in the width side of the original image, of shape B\n",
        "  - h_pixel: Number of pixels in the height side of the original image, of shape B\n",
        "  - w_amap: Number of pixels in the width side of the activation map, scalar\n",
        "  - h_amap: Number of pixels in the height side of the activation map, scalar\n",
        "  - mode: Whether transfer from the original image to activation map ('p2a') or\n",
        "          the opposite ('a2p')\n",
        "\n",
        "  Output:\n",
        "  - resized_bbox: Resized box coordinates, of the same shape as the input bbox\n",
        "  \"\"\"\n",
        "\n",
        "  assert mode in ('p2a', 'a2p'), 'invalid coordinate transformation mode!'\n",
        "  assert bbox.shape[-1] >= 4, 'the transformation is applied to the first 4 values of dim -1'\n",
        "\n",
        "  if bbox.shape[0] == 0: # corner cases\n",
        "    return bbox\n",
        "\n",
        "  resized_bbox = bbox.clone()\n",
        "  # could still work if the first dim of bbox is not batch size\n",
        "  # in that case, w_pixel and h_pixel will be scalars\n",
        "  resized_bbox = resized_bbox.view(bbox.shape[0], -1, bbox.shape[-1])\n",
        "  invalid_bbox_mask = (resized_bbox == -1) # indicating invalid bbox\n",
        "\n",
        "  if mode == 'p2a':\n",
        "    # pixel to activation\n",
        "    width_ratio = w_pixel * 1. / w_amap\n",
        "    height_ratio = h_pixel * 1. / h_amap\n",
        "    resized_bbox[:, :, [0, 2]] /= width_ratio.view(-1, 1, 1)\n",
        "    resized_bbox[:, :, [1, 3]] /= height_ratio.view(-1, 1, 1)\n",
        "  else:\n",
        "    # activation to pixel\n",
        "    width_ratio = w_pixel * 1. / w_amap\n",
        "    height_ratio = h_pixel * 1. / h_amap\n",
        "    resized_bbox[:, :, [0, 2]] *= width_ratio.view(-1, 1, 1)\n",
        "    resized_bbox[:, :, [1, 3]] *= height_ratio.view(-1, 1, 1)\n",
        "\n",
        "  resized_bbox.masked_fill_(invalid_bbox_mask, -1)\n",
        "  resized_bbox.resize_as_(bbox)\n",
        "  return resized_bbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48Cocwotg9Ly"
      },
      "source": [
        "## Data Visualizer\n",
        "\n",
        "image에서의 box visualization을 위한 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm3vPcQnhmRU"
      },
      "outputs": [],
      "source": [
        "def data_visualizer(img, idx_to_class, bbox=None, pred=None):\n",
        "  \"\"\"\n",
        "  Data visualizer on the original image. Support both GT box input and proposal input.\n",
        "\n",
        "  Input:\n",
        "  - img: PIL Image input\n",
        "  - idx_to_class: Mapping from the index (0-19) to the class name\n",
        "  - bbox: GT bbox (in red, optional), a tensor of shape Nx5, where N is\n",
        "          the number of GT boxes, 5 indicates (x_tl, y_tl, x_br, y_br, class)\n",
        "  - pred: Predicted bbox (in green, optional), a tensor of shape N'x6, where\n",
        "          N' is the number of predicted boxes, 6 indicates\n",
        "          (x_tl, y_tl, x_br, y_br, class, object confidence score)\n",
        "  \"\"\"\n",
        "\n",
        "  img_copy = np.array(img).astype('uint8')\n",
        "\n",
        "  if bbox is not None:\n",
        "    for bbox_idx in range(bbox.shape[0]):\n",
        "      one_bbox = bbox[bbox_idx][:4]\n",
        "      cv2.rectangle(img_copy, (int(one_bbox[0]), int(one_bbox[1])), (int(one_bbox[2]),\n",
        "                  int(one_bbox[3])), (255, 0, 0), 2)\n",
        "      if bbox.shape[1] > 4: # if class info provided\n",
        "        obj_cls = idx_to_class[bbox[bbox_idx][4].item()]\n",
        "        cv2.putText(img_copy, '%s' % (obj_cls),\n",
        "                  (int(one_bbox[0]), int(one_bbox[1])+15),\n",
        "                  cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n",
        "\n",
        "  if pred is not None:\n",
        "    for bbox_idx in range(pred.shape[0]):\n",
        "      one_bbox = pred[bbox_idx][:4]\n",
        "      cv2.rectangle(img_copy, (int(one_bbox[0]), int(one_bbox[1])), (int(one_bbox[2]),\n",
        "                  int(one_bbox[3])), (0, 255, 0), 2)\n",
        "\n",
        "      if pred.shape[1] > 4: # if class and conf score info provided\n",
        "        obj_cls = idx_to_class[pred[bbox_idx][4].item()]\n",
        "        conf_score = pred[bbox_idx][5].item()\n",
        "        cv2.putText(img_copy, '%s, %.2f' % (obj_cls, conf_score),\n",
        "                    (int(one_bbox[0]), int(one_bbox[1])+15),\n",
        "                    cv2.FONT_HERSHEY_PLAIN, 1.0, (0, 0, 255), thickness=1)\n",
        "\n",
        "  plt.imshow(img_copy)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4WmocEyiXWa"
      },
      "source": [
        "### Visualize PASCAL VOC 2007\n",
        "PASCAL VOC 2007 training set에 있는 이미지를 visualize해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld1s28Z4fyL5"
      },
      "outputs": [],
      "source": [
        "# default examples for visualization\n",
        "fix_random_seed(0)\n",
        "batch_size = 3\n",
        "sampled_idx = torch.linspace(0, len(train_dataset)-1, steps=batch_size).long()\n",
        "\n",
        "# get the size of each image first\n",
        "h_list = []\n",
        "w_list = []\n",
        "img_list = [] # list of images\n",
        "MAX_NUM_BBOX = 40\n",
        "box_list = torch.LongTensor(batch_size, MAX_NUM_BBOX, 5).fill_(-1) # PADDED GT boxes\n",
        "\n",
        "for idx, i in enumerate(sampled_idx):\n",
        "  # hack to get the original image so we don't have to load from local again...\n",
        "  img, ann = train_dataset.__getitem__(i)\n",
        "  img_list.append(img)\n",
        "\n",
        "  all_bbox = ann['annotation']['object']\n",
        "  if type(all_bbox) == dict:\n",
        "    all_bbox = [all_bbox]\n",
        "  for bbox_idx, one_bbox in enumerate(all_bbox):\n",
        "    bbox = one_bbox['bndbox']\n",
        "    obj_cls = one_bbox['name']\n",
        "    box_list[idx][bbox_idx] = torch.LongTensor([int(bbox['xmin']), int(bbox['ymin']),\n",
        "      int(bbox['xmax']), int(bbox['ymax']), class_to_idx[obj_cls]])\n",
        "\n",
        "  # get sizes\n",
        "  img = np.array(img)\n",
        "  w_list.append(img.shape[1])\n",
        "  h_list.append(img.shape[0])\n",
        "\n",
        "w_list = torch.as_tensor(w_list, **to_float_cuda)\n",
        "h_list = torch.as_tensor(h_list, **to_float_cuda)\n",
        "box_list = torch.as_tensor(box_list, **to_float_cuda)\n",
        "resized_box_list = coord_trans(box_list, w_list, h_list, mode='p2a') # on activation map coordinate system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v04D-gwEiWqY"
      },
      "outputs": [],
      "source": [
        "# visualize GT boxes\n",
        "for i in range(len(img_list)):\n",
        "  valid_box = sum([1 if j != -1 else 0 for j in box_list[i][:, 0]])\n",
        "  data_visualizer(img_list[i], idx_to_class, box_list[i][:valid_box])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAa1Kvl2P_2k"
      },
      "source": [
        "## Detector Backbone Network\n",
        "이번 과제에서는 [MobileNet v2](https://pytorch.org/hub/pytorch_vision_mobilenet_v2/)를 image feature extraction을 위한 backbone 네트워크로 사용할 예정입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLmU_CiURha7"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "  \"\"\"\n",
        "  Image feature extraction with MobileNet.\n",
        "  \"\"\"\n",
        "  def __init__(self, reshape_size=224, pooling=False, verbose=False):\n",
        "    super().__init__()\n",
        "\n",
        "    from torchvision import models\n",
        "    from torchsummary import summary\n",
        "\n",
        "    self.mobilenet = models.mobilenet_v2(pretrained=True)\n",
        "    self.mobilenet = nn.Sequential(*list(self.mobilenet.children())[:-1]) # Remove the last classifier\n",
        "\n",
        "    # average pooling\n",
        "    if pooling:\n",
        "      self.mobilenet.add_module('LastAvgPool', nn.AvgPool2d(math.ceil(reshape_size/32.))) # input: N x 1280 x 7 x 7\n",
        "\n",
        "    for i in self.mobilenet.named_parameters():\n",
        "      i[1].requires_grad = True # fine-tune all\n",
        "\n",
        "    if verbose:\n",
        "      summary(self.mobilenet.cuda(), (3, reshape_size, reshape_size))\n",
        "\n",
        "  def forward(self, img, verbose=False):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - img: Batch of resized images, of shape Nx3x224x224\n",
        "\n",
        "    Outputs:\n",
        "    - feat: Image feature, of shape Nx1280 (pooled) or Nx1280x7x7\n",
        "    \"\"\"\n",
        "    num_img = img.shape[0]\n",
        "\n",
        "    img_prepro = img\n",
        "\n",
        "    feat = []\n",
        "    process_batch = 500\n",
        "    for b in range(math.ceil(num_img/process_batch)):\n",
        "      feat.append(self.mobilenet(img_prepro[b*process_batch:(b+1)*process_batch]\n",
        "                              ).squeeze(-1).squeeze(-1)) # forward and squeeze\n",
        "    feat = torch.cat(feat)\n",
        "\n",
        "    if verbose:\n",
        "      print('Output feature shape: ', feat.shape)\n",
        "\n",
        "    return feat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHeRMCcjx6v0"
      },
      "source": [
        "MobileNet v2 내부를 살펴봅시다. 3x224x224 이미지 입력이 있다고 가정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pV0Lau_yDwX"
      },
      "outputs": [],
      "source": [
        "model = FeatureExtractor(verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz32q025joxy"
      },
      "source": [
        "## Anchor and Proposal\n",
        "\n",
        "강의 내용 중 소개해드렸듯이, Faster R-CNN에는 *anchor box*라고 불리는 개념이 도입됩니다. 이는 추후 one-stage detector인 YOLO v2 등의 모델에서도 사용됩니다.\n",
        "\n",
        "이미지를 backbone network에 거치도록 하고 나서, 우리는 shape이 $(C, 7, 7)$인 feature map을 얻게 될 것입니다. 이는 C-dimensional feature의 7x7 grid로 해석할 수 있습니다. grid의 각 point 별로 우리는 다른 크기와 모양의 $A$개의 anchor box를 고려할 예정입니다. 각 anchor box 별로 우리는 이 box의 objectness를 판단하여 해당 box가 object/background 중 어디에 해당하는지 분류합니다. 총 anchor box의 수는 $(A, 7, 7)$이 될 것이고, 우리는 이에 convolutional layer들을 거쳐 classification score를 예측할 것입니다.\n",
        "\n",
        "backbone activation feature map에 3x3 conv layer와 같은 작은 크기의 CNN architecture를 적용시킵니다. 이 3x3 conv를 *sliding window*라고 부릅니다. 각 location (i.e. 7x7 activation cell의 각 위치의 중심)에서 우리는 다수의  region *proposals*을 예측할 것이며, $A=9$가 될 것입니다.\n",
        "\n",
        "이 이후에 우리는 각 proposal에 대한 objectness를 추정하는 A-D score를 output으로 하는 object proposal layer, A개의 box의 좌표들에 대해 4A-D output을 만들어내는 bounding box regression layer, 그리고 모든 A개의 anchor가 공유하는 object category에 대한 score를 나타내는 20-D output을 만들어내는 region classification layer를 적용시킬 예정입니다. A개의 proposal들은 각각 A개의 anchor에 대해 상대적으로 parameterize되어있으며, 각 anchor는 sliding window를 중심으로 다양한 크기로 만들어집니다 (e.g., 1x1, 3x3, 5x5). anchor shape들의 list는 다음 셀에 주어집니다.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etBYc7rbj35F"
      },
      "source": [
        "### Anchor shapes\n",
        "\n",
        "anchor box들의 shape는 hyperparameter입니다. 일부 논문들에서는 anchor shape를 ground-truth box size를 clustering하여 정하기도 하지만, 이번 과제에는 단순하게 pre-define하도록 하겠습니다.\n",
        "\n",
        "original image에 대한 activation cell의 receptive field는 sliding window보다 클 수 있으므로 anchor의 크기는 3x3 sliding window보다 클 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5w-EUJekJj-"
      },
      "outputs": [],
      "source": [
        "# Declare variables for anchor priors, a Ax2 Tensor where A is the number of anchors.\n",
        "# Hand-picked, same as our two-stage detector.\n",
        "anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]], **to_float_cuda)\n",
        "print(anchor_list.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uochvAlqkgr8"
      },
      "source": [
        "### Activation Grid Generator\n",
        "\n",
        "7x7 backbone feature의 grid의 각 위치에 대한 anchor를 구하기 위해서 우리는 7x7 grid의 각 cell의 중심의 위치를 계산해야 합니다.\n",
        "\n",
        "다음 함수를 통해 center coordinate를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fC9-TKRykof7"
      },
      "outputs": [],
      "source": [
        "def GenerateGrid(batch_size, w_amap=7, h_amap=7, dtype=torch.float32, device='cuda'):\n",
        "  \"\"\"\n",
        "  Return a grid cell given a batch size (center coordinates).\n",
        "\n",
        "  Inputs:\n",
        "  - batch_size, B\n",
        "  - w_amap: or W', width of the activation map (number of grids in the horizontal dimension)\n",
        "  - h_amap: or H', height of the activation map (number of grids in the vertical dimension)\n",
        "  - W' and H' are always 7 in our case while w and h might vary.\n",
        "\n",
        "  Outputs:\n",
        "  grid: A float32 tensor of shape (B, H', W', 2) giving the (x, y) coordinates\n",
        "        of the centers of each feature for a feature map of shape (B, D, H', W')\n",
        "  \"\"\"\n",
        "  w_range = torch.arange(0, w_amap, dtype=dtype, device=device) + 0.5\n",
        "  h_range = torch.arange(0, h_amap, dtype=dtype, device=device) + 0.5\n",
        "\n",
        "  w_grid_idx = w_range.unsqueeze(0).repeat(h_amap, 1)\n",
        "  h_grid_idx = h_range.unsqueeze(1).repeat(1, w_amap)\n",
        "  grid = torch.stack([w_grid_idx, h_grid_idx], dim=-1)\n",
        "  grid = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)\n",
        "\n",
        "  return grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCHpDXV0sdEX"
      },
      "outputs": [],
      "source": [
        "# visualization\n",
        "# simply create an activation grid where the cells are in green and the centers in red\n",
        "# you should see the entire image divided by a 7x7 grid, with no gaps on the edges\n",
        "\n",
        "grid_list = GenerateGrid(w_list.shape[0])\n",
        "\n",
        "center = torch.cat((grid_list, grid_list), dim=-1)\n",
        "grid_cell = center.clone()\n",
        "grid_cell[:, :, :, [0, 1]] -= 1. / 2.\n",
        "grid_cell[:, :, :, [2, 3]] += 1. / 2.\n",
        "center = coord_trans(center, w_list, h_list)\n",
        "grid_cell = coord_trans(grid_cell, w_list, h_list)\n",
        "\n",
        "for img, anc, grid in zip(img_list, center, grid_cell):\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4), grid.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0knLi5KkxoS"
      },
      "source": [
        "### Anchor Generator\n",
        "이제 anchor box의 position을 구해봅시다.\n",
        "\n",
        "anchor coordinate를 다음과 같이 표기하도록 하겠습니다.\n",
        "\n",
        "($x_{tl}^a$, $y_{tl}^a$, $x_{br}^a$, $y_{br}^a$)\n",
        "\n",
        "이는 top-left corner와 bottom-right corner의 좌표값이고, 아래 함수는 모든 anchor를 주어진 cell과 anchor shape list에 따른 모든 anchor를 return합니다.\n",
        "\n",
        "**anchor의 center와 grid cell center는 동일합니다.**\n",
        "\n",
        "TODO 부분을 직접 채워보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AzuHo-WRrew-"
      },
      "outputs": [],
      "source": [
        "def GenerateAnchor(anc, grid):\n",
        "  \"\"\"\n",
        "  Anchor generator.\n",
        "\n",
        "  Inputs:\n",
        "  - anc: Tensor of shape (A, 2) giving the shapes of anchor boxes to consider at\n",
        "    each point in the grid. anc[a] = (w, h) gives the width and height of the\n",
        "    a'th anchor shape.\n",
        "  - grid: Tensor of shape (B, H', W', 2) giving the (x, y) coordinates of the\n",
        "    center of each feature from the backbone feature map. This is the tensor\n",
        "    returned from GenerateGrid.\n",
        "\n",
        "  Outputs:\n",
        "  - anchors: Tensor of shape (B, A, H', W', 4) giving the positions of all\n",
        "    anchor boxes for the entire image. anchors[b, a, h, w] is an anchor box\n",
        "    centered at grid[b, h, w], whose shape is given by anc[a]; we parameterize\n",
        "    boxes as anchors[b, a, h, w] = (x_tl, y_tl, x_br, y_br), where (x_tl, y_tl)\n",
        "    and (x_br, y_br) give the xy coordinates of the top-left and bottom-right\n",
        "    corners of the box.\n",
        "  \"\"\"\n",
        "  anchors = None\n",
        "  ##############################################################################\n",
        "  # TODO: Given a set of anchor shapes and a grid cell on the activation map,  #\n",
        "  # generate all the anchor coordinates for each image. Support batch input.   #\n",
        "  ##############################################################################\n",
        "  pass\n",
        "  ##############################################################################\n",
        "  #                               END OF YOUR CODE                             #\n",
        "  ##############################################################################\n",
        "\n",
        "  return anchors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9an-jXEr62yq"
      },
      "source": [
        "여러분이 구현을 맞게 했다면, 아래에서 1e-8 단위 혹은 그보다 작은 error 값을 가질 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gffaPg4Dsfux"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list)\n",
        "assert anc_list.shape == torch.Size([3, 9, 7, 7, 4]), 'shape mismatch!'\n",
        "\n",
        "expected_anc_list_mean = torch.tensor([[-1.25000000, -0.87500000,  2.25000000,  1.87500000],\n",
        "                                       [ 1.75000000, -0.87500000,  5.25000000,  1.87500000],\n",
        "                                       [ 4.75000000, -0.87500000,  8.25000000,  1.87500000],\n",
        "                                       [-1.25000000,  1.12500000,  2.25000000,  3.87500000],\n",
        "                                       [ 1.75000000,  1.12500000,  5.25000000,  3.87500000],\n",
        "                                       [ 4.75000000,  1.12500000,  8.25000000,  3.87500000],\n",
        "                                       [-1.25000000,  3.12500000,  2.25000000,  5.87500000],\n",
        "                                       [ 1.75000000,  3.12500000,  5.25000000,  5.87500000],\n",
        "                                       [ 4.75000000,  3.12500000,  8.25000000,  5.87500000],\n",
        "                                       [-1.25000000,  5.12500000,  2.25000000,  7.87500000],\n",
        "                                       [ 1.75000000,  5.12500000,  5.25000000,  7.87500000],\n",
        "                                       [ 4.75000000,  5.12500000,  8.25000000,  7.87500000]], **to_float_cuda)\n",
        "print('rel error: ', rel_error(expected_anc_list_mean, anc_list[0, [1,3,6,8], ::2, ::3, :].view(-1, 12, 4).mean(0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbxTvazYyV7P"
      },
      "outputs": [],
      "source": [
        "# visualization\n",
        "print('*'*80)\n",
        "print('All nine anchors should be exactly centered:')\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list[:, 3:4, 3:4])\n",
        "for img, anc in zip(img_list, coord_trans(anc_list, w_list, h_list)):\n",
        "  print(anc.shape)\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4))\n",
        "\n",
        "print('*'*80)\n",
        "print('All anchors of the image (cluttered):')\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list) # all\n",
        "for img, anc in zip(img_list, coord_trans(anc_list, w_list, h_list)):\n",
        "  print(anc.shape)\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCzcNX3Zsi3n"
      },
      "source": [
        "### Proposal Generator\n",
        "\n",
        "만약 우리가 object들의 위치를 제안하기 위해 anchor를 그대로 사용한다면, 우리는 image에서 9x7x7=441개의 region만 cover할 수 있을 겁니다. 만약 object가 이 441개의 anchor의 region 안에 있지 않다면 어떻게 해야 할까요?\n",
        "\n",
        "그렇기에 anchor box를 *region proposal*로 변환해주는 transformation을 예측하는 작업을 진행해야 합니다.\n",
        "\n",
        "$(x_{tl}^a, y_{tl}^a, x_{br}^a, y_{br}^a)$의 anchor box가 있다고 할 때, transformation formulation을 더 쉽게 하기 위해 이제부터는 이를 box 중심의 x/y coordinate와 width, height: $(x_c^a,y_c^a,w^a,h^a)$로 표현할 것입니다.\n",
        "\n",
        "$(x_c^a,y_c^a,w^a,h^a)$의 anchor box에 대해 network가 *transformation* $(t^x, t^y, t^w, t^h)$을 예측하고, 이 transformation을 적용시킨 *region proposal*의 center, width and height $(x_c^p,y_c^p,w^p,h^p)$를 구합니다.\n",
        "\n",
        "참고로 YOLO와 Faster R-CNN은 anchor를 proposal로 transform하는 과정에서 약간 다른 식을 사용합니다. 각 경우의 transformation에 대해 여러분은 이를 모두 구현하시면 됩니다.\n",
        "\n",
        "### YOLO\n",
        "YOLO는 $t^x$와 $t^y$의 범위가 $-0.5\\leq t^x,t^y\\leq 0.5$라고 가정하며, 반면 $t^w$와 $t^h$의 범위는 실수 전체 $(-\\infty, \\infty)$라고 가정합니다. 변환 수식은 다음과 같습니다:\n",
        "- $x_c^p = x_c^a + t^x$\n",
        "- $y_c^p = y_c^a + t^y$\n",
        "- $w^p = w_a exp(t^w)$\n",
        "- $h^p = h_a exp(t^h)$\n",
        "\n",
        "### Faster R-CNN\n",
        "Faster R-CNN은 모든 parameter $t^x, t^y, t^w, t_h$의 범위가 실수 전체 $(-\\infty, \\infty)$입니다. 수식은 다음과 같습니다:\n",
        "- $x_c^p = x_c^a + t^xw^a$\n",
        "- $y_c^p = y_c^p + t^yh^a$\n",
        "- $w^p = w_a exp(t^w)$\n",
        "- $h^p = h_a exp(t^h)$\n",
        "\n",
        "\n",
        "### Training\n",
        "\n",
        "학습을 위해서 우리는 ground-truth transformation $(\\hat{t^x}, \\hat{t^y}, \\hat{t^w}, \\hat{t^h})$을 계산합니다. 이는 anchor box $(x_c^a,y_c^a,w^a,h^a)$를  the ground-truth box $(x_c^{gt},y_c^{gt},w^{gt},h^{gt})$로 변환하는 transformation입니다.\n",
        "\n",
        "이후 우리는 regression loss를 통해 predicted transform $(t^x, t^y, t^w, t^h)$ 과 ground-truth transform 사이의 차이를 penalize합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mx9BlyIXspZy"
      },
      "outputs": [],
      "source": [
        "def GenerateProposal(anchors, offsets, method='YOLO'):\n",
        "  \"\"\"\n",
        "  Proposal generator.\n",
        "\n",
        "  Inputs:\n",
        "  - anchors: Anchor boxes, of shape (B, A, H', W', 4). Anchors are represented\n",
        "    by the coordinates of their top-left and bottom-right corners.\n",
        "  - offsets: Transformations of shape (B, A, H', W', 4) that will be used to\n",
        "    convert anchor boxes into region proposals. The transformation\n",
        "    offsets[b, a, h, w] = (tx, ty, tw, th) will be applied to the anchor\n",
        "    anchors[b, a, h, w]. For YOLO, assume that tx and ty are in the range\n",
        "    (-0.5, 0.5).\n",
        "  - method: Which transformation formula to use, either 'YOLO' or 'FasterRCNN'\n",
        "\n",
        "  Outputs:\n",
        "  - proposals: Region proposals of shape (B, A, H', W', 4), represented by the\n",
        "    coordinates of their top-left and bottom-right corners. Applying the\n",
        "    transform offsets[b, a, h, w] to the anchor [b, a, h, w] should give the\n",
        "    proposal proposals[b, a, h, w].\n",
        "\n",
        "  \"\"\"\n",
        "  assert(method in ['YOLO', 'FasterRCNN'])\n",
        "  proposals = None\n",
        "  ##############################################################################\n",
        "  # TODO: Given anchor coordinates and the proposed offset for each anchor,    #\n",
        "  # compute the proposal coordinates using the transformation formulas above.  #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  pass\n",
        "  ##############################################################################\n",
        "  #                               END OF YOUR CODE                             #\n",
        "  ##############################################################################\n",
        "\n",
        "  return proposals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IglWagADIb6"
      },
      "source": [
        "올바른 구현을 하셨다면 아래 cell을 작동시키면 1e-7 단위 혹은 그보다 작은 error 값을 갖게 될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beEUhlCHtFAN"
      },
      "outputs": [],
      "source": [
        "print('-' * 80)\n",
        "print('Sanity check for YOLO.')\n",
        "yolo_offset_list = torch.cat([torch.ones_like(anc_list[:, :, :, :, 0:2]).fill_(.5), torch.ones_like(anc_list[:, :, :, :, 2:4])], dim=-1)\n",
        "yolo_proposal_list = GenerateProposal(anc_list, yolo_offset_list, 'YOLO') # no scaling\n",
        "\n",
        "print('1. Center moved by ~0.5 cell')\n",
        "print('rel error: ', rel_error(anc_list[0, 0, 0, :, 0:2] + (anc_list[0, 0, 0, :, 2:4] - anc_list[0, 0, 0, :, 0:2])/2.0 + 0.5, \\\n",
        "                               (yolo_proposal_list[0, 0, 0, :, 0:2] + (yolo_proposal_list[0, 0, 0, :, 2:4] - yolo_proposal_list[0, 0, 0, :, 0:2]) / 2.0)))\n",
        "\n",
        "print('2. w/h changed by e')\n",
        "print('rel error: ', rel_error((anc_list[0, 0, 0, :, 2:4] - anc_list[0, 0, 0, :, 0:2]) * torch.exp(torch.ones_like(anc_list[0, 0, 0, :, 0:2])), \\\n",
        "      (yolo_proposal_list[0, 0, 0, :, 2:4] - yolo_proposal_list[0, 0, 0, :, 0:2])))\n",
        "\n",
        "\n",
        "print('-' * 80)\n",
        "print('Sanity check for FasterRCNN.')\n",
        "rcnn_offset_list = torch.ones_like(anc_list)\n",
        "rcnn_proposal_list = GenerateProposal(anc_list, rcnn_offset_list, 'FasterRCNN')\n",
        "\n",
        "print('1. x/y shifted by wh')\n",
        "print('rel error: ', rel_error(anc_list[0, 0, 0, :, 0:2] + (anc_list[0, 0, 0, :, 2:4] - anc_list[0, 0, 0, :, 0:2]) * 3.0 /2.0, \\\n",
        "      (rcnn_proposal_list[0, 0, 0, :, 0:2] + (rcnn_proposal_list[0, 0, 0, :, 2:4] - rcnn_proposal_list[0, 0, 0, :, 0:2]) / 2.0)))\n",
        "\n",
        "print('2. w/h should changed by e')\n",
        "print('rel error: ', rel_error((anc_list[0, 0, 0, :, 2:4] - anc_list[0, 0, 0, :, 0:2]) * torch.exp(torch.ones_like(anc_list[0, 0, 0, :, 0:2])), \\\n",
        "      (rcnn_proposal_list[0, 0, 0, :, 2:4] - rcnn_proposal_list[0, 0, 0, :, 0:2])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYzP-8tGo6bp"
      },
      "source": [
        "추가적인 sanity check를 위해 anchor (red)와 이에 대해서 $(0.5, 0.5, 0, 0)$의 transformation을 진행한 proposal (green)을 visualize해봅시다. proposal은 반드시 우측 아래 방향으로 움직여야 할 것입니다. (YOLO formulation에서)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQZH2b7fhS49"
      },
      "outputs": [],
      "source": [
        "yolo_offset_list = torch.cat([torch.ones_like(anc_list[:, :, :, :, 0:2]).fill_(.5), torch.zeros_like(anc_list[:, :, :, :, 2:4])], dim=-1)\n",
        "yolo_proposal_list = GenerateProposal(anc_list, yolo_offset_list, 'YOLO')\n",
        "\n",
        "for img, anc, prop in zip(img_list, coord_trans(anc_list[:, 0:1, 3:4, 3:4, :], w_list, h_list), \\\n",
        "                          coord_trans(yolo_proposal_list[:, 0:1, 3:4, 3:4, :], w_list, h_list)):\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4), prop.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvR1zYcMpdYU"
      },
      "source": [
        "$(0, 0, 1, 1)$을 적용한다면 proposal은 동일한 center 위치에 크기만 커진 형태일 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LtrWZCeyYVS"
      },
      "outputs": [],
      "source": [
        "yolo_offset_list = torch.cat([torch.zeros_like(anc_list[:, :, :, :, 0:2]), torch.ones_like(anc_list[:, :, :, :, 2:4]).fill_(1.)], dim=-1)\n",
        "yolo_proposal_list = GenerateProposal(anc_list, yolo_offset_list, 'YOLO')\n",
        "\n",
        "for img, anc, prop in zip(img_list, coord_trans(anc_list[:, 0:1, 3:4, 3:4, :], w_list, h_list), \\\n",
        "                          coord_trans(yolo_proposal_list[:, 0:1, 3:4, 3:4, :], w_list, h_list)):\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4), prop.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7WJQkOkp6E5"
      },
      "source": [
        "Faster R-CNN에 대해서도 이를 확인해봅시다. $(1, 1, 0, 0)$의 transformation을 적용한다면 proposal은 anchor의 크기와 같은 양만큼 우측 아래로 shift되어야 할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk6FWCcYh1hZ"
      },
      "outputs": [],
      "source": [
        "# visualization (shift by wh, Faster R-CNN)\n",
        "# anchors in red and proposals in green\n",
        "rcnn_offset_list = torch.cat([torch.ones_like(anc_list[:, :, :, :, 0:2]), torch.zeros_like(anc_list[:, :, :, :, 2:4])], dim=-1)\n",
        "rcnn_proposal_list = GenerateProposal(anc_list, rcnn_offset_list, 'FasterRCNN')\n",
        "\n",
        "for img, anc, prop in zip(img_list, coord_trans(anc_list[:, 0:1, 3:4, 3:4, :], w_list, h_list), \\\n",
        "                          coord_trans(rcnn_proposal_list[:, 0:1, 3:4, 3:4, :], w_list, h_list)):\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4), prop.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ex-G6inJqOgv"
      },
      "source": [
        "$(0, 0, 1, 1)$을 적용한다면 center는 같고 e의 승수 크기만큼 커진 proposal을 얻게 될 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzDVtYz1hjVR"
      },
      "outputs": [],
      "source": [
        "# visualization (no shift and then scale by e, Faster R-CNN)\n",
        "# anchors in red and proposals in green\n",
        "rcnn_offset_list = torch.cat([torch.zeros_like(anc_list[:, :, :, :, 0:2]), torch.ones_like(anc_list[:, :, :, :, 2:4]).fill_(1)], dim=-1)\n",
        "rcnn_proposal_list = GenerateProposal(anc_list, rcnn_offset_list, 'FasterRCNN')\n",
        "\n",
        "for img, anc, prop in zip(img_list, coord_trans(anc_list[:, 0:1, 3:4, 3:4, :], w_list, h_list), \\\n",
        "                          coord_trans(rcnn_proposal_list[:, 0:1, 3:4, 3:4, :], w_list, h_list)):\n",
        "  data_visualizer(img, idx_to_class, anc.reshape(-1, 4), prop.reshape(-1, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lBcOWGWtNka"
      },
      "source": [
        "## Prediction Networks\n",
        "해당 모듈은 아래 그림과 같이 prediction score를 output으로 내놓습니다. 아래 셀에서 activated/negative anchor를 결정하는 코드는 구현되어 있으나, 여러분이 직접 IoU를 계산하는 함수는 구현해야 합니다. 또한 loss function을 계산해야 하며, 이는 confidence score regression / bounding box offsets regression / object classification 세 가지의 part로 구성됩니다.\n",
        "\n",
        "![pred_scores](https://miro.medium.com/max/1055/1*YG6heD55fEmZeUKRSlsqlA.png)\n",
        "\n",
        "\n",
        "Image credit: [towardsdatascience](https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89). 해당 예제에서는 $A=2$입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8HqHqojtXhE"
      },
      "source": [
        "### Intersection Over Union (IoU)\n",
        "\n",
        "IoU는 강의 시간에 다뤘던 거 다들 기억하시죠? 자세한 정의가 기억이 안 나신다면 아래 자료 35-39 페이지를 참고해주세요.\n",
        "https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture15.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJOBdG2ltgT5"
      },
      "outputs": [],
      "source": [
        "def IoU(proposals, bboxes):\n",
        "  \"\"\"\n",
        "  Compute intersection over union between sets of bounding boxes.\n",
        "\n",
        "  Inputs:\n",
        "  - proposals: Proposals of shape (B, A, H', W', 4)\n",
        "  - bboxes: Ground-truth boxes from the DataLoader of shape (B, N, 5).\n",
        "    Each ground-truth box is represented as tuple (x_lr, y_lr, x_rb, y_rb, class).\n",
        "    If image i has fewer than N boxes, then bboxes[i] will be padded with extra\n",
        "    rows of -1.\n",
        "\n",
        "  Outputs:\n",
        "  - iou_mat: IoU matrix of shape (B, A*H'*W', N) where iou_mat[b, i, n] gives\n",
        "    the IoU between one element of proposals[b] and bboxes[b, n].\n",
        "\n",
        "  For this implementation you DO NOT need to filter invalid proposals or boxes;\n",
        "  in particular you don't need any special handling for bboxxes that are padded\n",
        "  with -1.\n",
        "  \"\"\"\n",
        "  iou_mat = None\n",
        "  ##############################################################################\n",
        "  # TODO: Compute the Intersection over Union (IoU) on proposals and GT boxes. #\n",
        "  # No need to filter invalid proposals/bboxes (i.e., allow region area <= 0). #\n",
        "  # You need to ensure your implementation is efficient (no for loops).        #\n",
        "  # HINT:                                                                      #\n",
        "  # IoU = Area of Intersection / Area of Union, where\n",
        "  # Area of Union = Area of Proposal + Area of BBox - Area of Intersection     #\n",
        "  # and the Area of Intersection can be computed using the top-left corner and #\n",
        "  # bottom-right corner of proposal and bbox. Think about their relationships. #\n",
        "  ##############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  pass\n",
        "  ##############################################################################\n",
        "  #                               END OF YOUR CODE                             #\n",
        "  ##############################################################################\n",
        "  return iou_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY52QCKlTBsR"
      },
      "source": [
        "구현이 잘 되었다면, error의 값이 1e-8 단위 혹은 그보다 작을 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRXHJXFIbbDs"
      },
      "outputs": [],
      "source": [
        "# simple sanity check\n",
        "width, height = torch.tensor([35, 35], **to_float_cuda), torch.tensor([40, 40], **to_float_cuda)\n",
        "sample_bbox = torch.tensor([[[1,1,11,11,0], [20,20,30,30,0]]], **to_float_cuda)\n",
        "sample_proposals = torch.tensor([[[[[5,5,15,15], [27,27,37,37]]]]], **to_float_cuda)\n",
        "\n",
        "result = IoU(sample_proposals, sample_bbox)\n",
        "\n",
        "# check 1\n",
        "expected_result = torch.tensor([[[0.21951219, 0.00000000],\n",
        "                                 [0.00000000, 0.04712042]]], **to_float_cuda)\n",
        "print('simple iou_mat error: ', rel_error(expected_result, result))\n",
        "\n",
        "# check 2\n",
        "iou_mat = IoU(anc_list, resized_box_list)\n",
        "expected_iou_mat = torch.tensor([0.11666405, 0.15146968, 0.02956639], **to_float_cuda)\n",
        "print('iou_mat error: ', rel_error(expected_iou_mat, iou_mat[:, :, 0].mean(1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNSwO-wDwzoQ"
      },
      "source": [
        "### Activated (positive) and negative anchors\n",
        "\n",
        "훈련 과정에서 우리는 각 anchor들에 대한 classification label을 결정하기 위해 ground-truth box들을 anchor들에 matching시켜야 합니다. 이는 어떤 anchor가 object를 포함하는지 혹은 background로 분류되어야 하는지를 결정해야 한다는 의미입니다. 이 부분은 아래에 구현이 되어 있습니다.\n",
        "\n",
        "아래 함수 설명과 구현을 자세히 읽어보시는 것을 매우 추천드립니다. input / output definition과 코드 구현을 잘 살펴보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7uXbDraMkHR"
      },
      "outputs": [],
      "source": [
        "def ReferenceOnActivatedAnchors(anchors, bboxes, grid, iou_mat, pos_thresh=0.7, neg_thresh=0.3, method='FasterRCNN'):\n",
        "  \"\"\"\n",
        "  Determine the activated (positive) and negative anchors for model training.\n",
        "\n",
        "  For YOLO - A grid cell is responsible for predicting a GT box if the center of\n",
        "  the box falls into that cell.\n",
        "  Implementation details: First compute manhattan distance between grid cell centers\n",
        "  (BxH’xW’) and GT box centers (BxN). This gives us a matrix of shape Bx(H'xW')xN and\n",
        "  perform torch.min(dim=1)[1] on it gives us the indexes indicating activated grids\n",
        "  responsible for GT boxes (convert to x and y). Among all the anchors associated with\n",
        "  the activate grids, the anchor with the largest IoU with the GT box is responsible to\n",
        "  predict (regress to) the GT box.\n",
        "  Note: One anchor might match multiple GT boxes.\n",
        "\n",
        "  For Faster R-CNN - Positive anchors are defined Any of the two\n",
        "  (i) the anchor/anchors with the highest IoU overlap with a GT box, or\n",
        "  (ii) an anchor that has an IoU overlap higher than 0.7 with any GT box.\n",
        "  Note: One anchor can match at most one GT box (the one with the largest IoU overlapping).\n",
        "\n",
        "  For both - We assign a negative label to a anchor if its IoU ratio is lower than\n",
        "  a threshold value for all GT boxes. Anchors that are neither positive nor negative\n",
        "  do not contribute to the training objective.\n",
        "\n",
        "  Main steps include:\n",
        "  i) Decide activated and negative anchors based on the IoU matrix.\n",
        "  ii) Compute GT confidence score/offsets/object class on the positive proposals.\n",
        "  iii) Compute GT confidence score on the negative proposals.\n",
        "\n",
        "  Inputs:\n",
        "  - anchors: Anchor boxes, of shape BxAxH’xW’x4\n",
        "  - bboxes: GT boxes of shape BxNx5, where N is the number of PADDED GT boxes,\n",
        "            5 indicates (x_{lr}^{gt}, y_{lr}^{gt}, x_{rb}^{gt}, y_{rb}^{gt}) and class index\n",
        "  - grid (float): A cell grid of shape BxH'xW'x2 where 2 indicate the (x, y) coord\n",
        "  - iou_mat: IoU matrix of shape Bx(AxH’xW’)xN\n",
        "  - pos_thresh: Positive threshold value\n",
        "  - neg_thresh: Negative threshold value\n",
        "  - method: Switch between 'YOLO' mode and 'FasterRCNN' mode\n",
        "\n",
        "  Outputs:\n",
        "  - activated_anc_ind: Index on activated anchors, of shape M, where M indicates the\n",
        "                       number of activated anchors\n",
        "  - negative_anc_ind: Index on negative anchors, of shape M\n",
        "  - GT_conf_scores: GT IoU confidence scores on activated anchors, of shape M\n",
        "  - GT_offsets: GT offsets on activated anchors, of shape Mx4. They are denoted as\n",
        "                \\hat{t^x}, \\hat{t^y}, \\hat{t^w}, \\hat{t^h} in the formulation earlier.\n",
        "  - GT_class: GT class category on activated anchors, essentially indexed from bboxes[:, :, 4],\n",
        "              of shape M\n",
        "  - activated_anc_coord: Coordinates on activated anchors (mainly for visualization purposes)\n",
        "  - negative_anc_coord: Coordinates on negative anchors (mainly for visualization purposes)\n",
        "  \"\"\"\n",
        "\n",
        "  assert(method in ['FasterRCNN', 'YOLO'])\n",
        "\n",
        "  B, A, h_amap, w_amap, _ = anchors.shape\n",
        "  N = bboxes.shape[1]\n",
        "\n",
        "  # activated/positive anchors\n",
        "  max_iou_per_anc, max_iou_per_anc_ind = iou_mat.max(dim=-1)\n",
        "  if method == 'FasterRCNN':\n",
        "    max_iou_per_box = iou_mat.max(dim=1, keepdim=True)[0]\n",
        "    activated_anc_mask = (iou_mat == max_iou_per_box) & (max_iou_per_box > 0)\n",
        "    activated_anc_mask |= (iou_mat > pos_thresh) # using the pos_thresh condition as well\n",
        "    # if an anchor matches multiple GT boxes, choose the box with the largest iou\n",
        "    activated_anc_mask = activated_anc_mask.max(dim=-1)[0] # Bx(AxH’xW’)\n",
        "    activated_anc_ind = torch.nonzero(activated_anc_mask.view(-1)).squeeze(-1)\n",
        "\n",
        "    # GT conf scores\n",
        "    GT_conf_scores = max_iou_per_anc[activated_anc_mask] # M\n",
        "\n",
        "    # GT class\n",
        "    box_cls = bboxes[:, :, 4].view(B, 1, N).expand((B, A*h_amap*w_amap, N))\n",
        "    GT_class = torch.gather(box_cls, -1, max_iou_per_anc_ind.unsqueeze(-1)).squeeze(-1) # M\n",
        "    GT_class = GT_class[activated_anc_mask].long()\n",
        "\n",
        "    bboxes_expand = bboxes[:, :, :4].view(B, 1, N, 4).expand((B, A*h_amap*w_amap, N, 4))\n",
        "    bboxes = torch.gather(bboxes_expand, -2, max_iou_per_anc_ind.unsqueeze(-1) \\\n",
        "      .unsqueeze(-1).expand(B, A*h_amap*w_amap, 1, 4)).view(-1, 4)\n",
        "    bboxes = bboxes[activated_anc_ind]\n",
        "  else:\n",
        "    bbox_mask = (bboxes[:, :, 0] != -1) # BxN, indicate invalid boxes\n",
        "    bbox_centers = (bboxes[:, :, 2:4] - bboxes[:, :, :2]) / 2. + bboxes[:, :, :2] # BxNx2\n",
        "\n",
        "    mah_dist = torch.abs(grid.view(B, -1, 2).unsqueeze(2) - bbox_centers.unsqueeze(1)).sum(dim=-1) # Bx(H'xW')xN\n",
        "    min_mah_dist = mah_dist.min(dim=1, keepdim=True)[0] # Bx1xN\n",
        "    grid_mask = (mah_dist == min_mah_dist).unsqueeze(1) # Bx1x(H'xW')xN\n",
        "\n",
        "    reshaped_iou_mat = iou_mat.view(B, A, -1, N)\n",
        "    anc_with_largest_iou = reshaped_iou_mat.max(dim=1, keepdim=True)[0] # Bx1x(H’xW’)xN\n",
        "    anc_mask = (anc_with_largest_iou == reshaped_iou_mat) # BxAx(H’xW’)xN\n",
        "    activated_anc_mask = (grid_mask & anc_mask).view(B, -1, N)\n",
        "    activated_anc_mask &= bbox_mask.unsqueeze(1)\n",
        "\n",
        "    # one anchor could match multiple GT boxes\n",
        "    activated_anc_ind = torch.nonzero(activated_anc_mask.view(-1)).squeeze(-1)\n",
        "    GT_conf_scores = iou_mat.view(-1)[activated_anc_ind]\n",
        "    bboxes = bboxes.view(B, 1, N, 5).repeat(1, A*h_amap*w_amap, 1, 1).view(-1, 5)[activated_anc_ind]\n",
        "    GT_class = bboxes[:, 4].long()\n",
        "    bboxes = bboxes[:, :4]\n",
        "    activated_anc_ind = (activated_anc_ind / activated_anc_mask.shape[-1]).long()\n",
        "\n",
        "  print('number of pos proposals: ', activated_anc_ind.shape[0])\n",
        "  activated_anc_coord = anchors.view(-1, 4)[activated_anc_ind]\n",
        "\n",
        "  # GT offsets\n",
        "  # bbox and anchor coordinates are x_tl, y_tl, x_br, y_br\n",
        "  # offsets are t_x, t_y, t_w, t_h\n",
        "  wh_offsets = torch.log((bboxes[:, 2:4] - bboxes[:, :2]) \\\n",
        "    / (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2]))\n",
        "\n",
        "  xy_offsets = (bboxes[:, :2] + bboxes[:, 2:4] - \\\n",
        "    activated_anc_coord[:, :2] - activated_anc_coord[:, 2:4]) / 2.\n",
        "\n",
        "  if method == \"FasterRCNN\":\n",
        "    xy_offsets /= (activated_anc_coord[:, 2:4] - activated_anc_coord[:, :2])\n",
        "  else:\n",
        "    assert torch.max(torch.abs(xy_offsets)) <= 0.5, \\\n",
        "      \"x and y offsets should be between -0.5 and 0.5! Got {}\".format( \\\n",
        "      torch.max(torch.abs(xy_offsets)))\n",
        "\n",
        "  GT_offsets = torch.cat((xy_offsets, wh_offsets), dim=-1)\n",
        "\n",
        "  # negative anchors\n",
        "  negative_anc_mask = (max_iou_per_anc < neg_thresh) # Bx(AxH’xW’)\n",
        "  negative_anc_ind = torch.nonzero(negative_anc_mask.view(-1)).squeeze(-1)\n",
        "  negative_anc_ind = negative_anc_ind[torch.randint(0, negative_anc_ind.shape[0], (activated_anc_ind.shape[0],))]\n",
        "  negative_anc_coord = anchors.view(-1, 4)[negative_anc_ind.view(-1)]\n",
        "\n",
        "  # activated_anc_coord and negative_anc_coord are mainly for visualization purposes\n",
        "  return activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, \\\n",
        "         activated_anc_coord, negative_anc_coord"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OHpu5SMXrio"
      },
      "source": [
        "구현이 잘 되었다면, error의 값이 1e-7 단위 혹은 그보다 작을 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK_USCuaXSzh"
      },
      "outputs": [],
      "source": [
        "# sanity check on YOLO (the one on Faster R-CNN is in A5-2)\n",
        "activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, \\\n",
        "  activated_anc_coord, negative_anc_coord = ReferenceOnActivatedAnchors(anc_list, resized_box_list, grid_list, iou_mat, neg_thresh=0.2, method='YOLO')\n",
        "\n",
        "expected_GT_conf_scores = torch.tensor([0.74538743, 0.72793430, 0.76044953, 0.37116671], **to_float_cuda)\n",
        "expected_GT_offsets = torch.tensor([[ 0.04900002,  0.35735703, -0.09431065,  0.19244696],\n",
        "                                    [-0.14700007,  0.37299442, -0.00250307,  0.25213102],\n",
        "                                    [-0.40600014,  0.09625626,  0.20863886, -0.07974572],\n",
        "                                    [ 0.15399981, -0.42933345, -0.03459148, -0.86750042]], **to_float_cuda)\n",
        "expected_GT_class = torch.tensor([ 6,  7, 19,  6], **to_long_cuda)\n",
        "print('conf scores error: ', rel_error(GT_conf_scores, expected_GT_conf_scores))\n",
        "print('offsets error: ', rel_error(GT_offsets, expected_GT_offsets))\n",
        "print('class prob error: ', rel_error(GT_class, expected_GT_class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsPZw2mTutWP"
      },
      "source": [
        "visualize도 해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2TuJNoCvUuqc"
      },
      "outputs": [],
      "source": [
        "# visualize activated and negative anchors\n",
        "anc_per_img = torch.prod(torch.tensor(anc_list.shape[1:-1]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Activated (positive) anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (activated_anc_ind >= idx * anc_per_img) & (activated_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} activated anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(activated_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Negative anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (negative_anc_ind >= idx * anc_per_img) & (negative_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} negative anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(negative_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW_Zek3_dgfF"
      },
      "source": [
        "### Prediction Network\n",
        "\n",
        "Prediction network는 backbone network로 부터 얻은 feature를 input으로 받아서 각 anchor에 대한 class score와 transformation을 output으로 내놓습니다.\n",
        "\n",
        "backbone feature의 7x7 grid의 각 position에서 prediction network는 `C`개의 object category들에 대한 `C`개의 classification score를 output으로 내놓습니다.\n",
        "\n",
        "추가적으로 각 position 별로 `A`개의 anchor들에 대해 prediction network는 transformation에 대한 4개의 number와 confidence score (이 값이 큰 양수일수록 anchor가 object를 포함할 확률이 높다는 것을 나타냅니다. 그리고 큰 음수라면 확률이 매우 낮다는 것입니다.) 를 예측해야 합니다.\n",
        "\n",
        "이 모든 output을 구한다면 7x7 grid features들은 각 `5A+C`개의 output을 가질 것이며, input tensor의 shape는 `(B, 1280, 7, 7)`, output은 `(B, 5A+C, 7, 7)`가 될 것입니다.\n",
        "\n",
        "우리는 이를 두 개의 `1x1` convolution layer를 input tensor에 적용시켜 얻어낼 것입니다.\n",
        "\n",
        "훈련 과정에서 모든 anchor box에 대한 loss를 계산하는 대신, positive/negative anchor의 subset들을 뽑아와서 계산합니다. Prediction network가 positive/negative anchor를 뽑아오는 과정도 수행할 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PredictionNetwork(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=128, num_anchors=9, num_classes=20, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_classes != 0 and num_anchors != 0)\n",
        "    self.num_classes = num_classes\n",
        "    self.num_anchors = num_anchors\n",
        "\n",
        "    ##############################################################################\n",
        "    # TODO: Set up a network that will predict outputs for all anchors. This     #\n",
        "    # network should have a 1x1 convolution with hidden_dim filters, followed    #\n",
        "    # by a Dropout layer with p=drop_ratio, a Leaky ReLU nonlinearity, and       #\n",
        "    # finally another 1x1 convolution layer to predict all outputs. You can      #\n",
        "    # use an nn.Sequential for this network, and store it in a member variable.  #\n",
        "    # HINT: The output should be of shape (B, 5*A+C, 7, 7), where                #\n",
        "    # A=self.num_anchors and C=self.num_classes.                                 #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def _extract_anchor_data(self, anchor_data, anchor_idx):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - anchor_data: Tensor of shape (B, A, D, H, W) giving a vector of length\n",
        "      D for each of A anchors at each point in an H x W grid.\n",
        "    - anchor_idx: int64 Tensor of shape (M,) giving anchor indices to extract\n",
        "\n",
        "    Returns:\n",
        "    - extracted_anchors: Tensor of shape (M, D) giving anchor data for each\n",
        "      of the anchors specified by anchor_idx.\n",
        "    \"\"\"\n",
        "    B, A, D, H, W = anchor_data.shape\n",
        "    anchor_data = anchor_data.permute(0, 1, 3, 4, 2).contiguous().view(-1, D)\n",
        "    extracted_anchors = anchor_data[anchor_idx]\n",
        "    return extracted_anchors\n",
        "\n",
        "  def _extract_class_scores(self, all_scores, anchor_idx):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - all_scores: Tensor of shape (B, C, H, W) giving classification scores for\n",
        "      C classes at each point in an H x W grid.\n",
        "    - anchor_idx: int64 Tensor of shape (M,) giving the indices of anchors at\n",
        "      which to extract classification scores\n",
        "\n",
        "    Returns:\n",
        "    - extracted_scores: Tensor of shape (M, C) giving the classification scores\n",
        "      for each of the anchors specified by anchor_idx.\n",
        "    \"\"\"\n",
        "    B, C, H, W = all_scores.shape\n",
        "    A = self.num_anchors\n",
        "    all_scores = all_scores.contiguous().permute(0, 2, 3, 1).contiguous()\n",
        "    all_scores = all_scores.view(B, 1, H, W, C).expand(B, A, H, W, C)\n",
        "    all_scores = all_scores.reshape(B * A * H * W, C)\n",
        "    extracted_scores = all_scores[anchor_idx]\n",
        "    return extracted_scores\n",
        "\n",
        "  def forward(self, features, pos_anchor_idx=None, neg_anchor_idx=None):\n",
        "    \"\"\"\n",
        "    Run the forward pass of the network to predict outputs given features\n",
        "    from the backbone network.\n",
        "\n",
        "    Inputs:\n",
        "    - features: Tensor of shape (B, in_dim, 7, 7) giving image features computed\n",
        "      by the backbone network.\n",
        "    - pos_anchor_idx: int64 Tensor of shape (M,) giving the indices of anchors\n",
        "      marked as positive. These are only given during training; at test-time\n",
        "      this should be None.\n",
        "    - neg_anchor_idx: int64 Tensor of shape (M,) giving the indices of anchors\n",
        "      marked as negative. These are only given at training; at test-time this\n",
        "      should be None.\n",
        "\n",
        "    The outputs from this method are different during training and inference.\n",
        "\n",
        "    During training, pos_anchor_idx and neg_anchor_idx are given and identify\n",
        "    which anchors should be positive and negative, and this forward pass needs\n",
        "    to extract only the predictions for the positive and negative anchors.\n",
        "\n",
        "    During inference, only features are provided and this method needs to return\n",
        "    predictions for all anchors.\n",
        "\n",
        "    Outputs (During training):\n",
        "    - conf_scores: Tensor of shape (2*M, 1) giving the predicted classification\n",
        "      scores for positive anchors and negative anchors (in that order).\n",
        "    - offsets: Tensor of shape (M, 4) giving predicted transformation for\n",
        "      positive anchors.\n",
        "    - class_scores: Tensor of shape (M, C) giving classification scores for\n",
        "      positive anchors.\n",
        "\n",
        "    Outputs (During inference):\n",
        "    - conf_scores: Tensor of shape (B, A, H, W) giving predicted classification\n",
        "      scores for all anchors.\n",
        "    - offsets: Tensor of shape (B, A, 4, H, W) giving predicted transformations\n",
        "      all all anchors.\n",
        "    - class_scores: Tensor of shape (B, C, H, W) giving classification scores for\n",
        "      each spatial position.\n",
        "    \"\"\"\n",
        "    conf_scores, offsets, class_scores = None, None, None\n",
        "    ############################################################################\n",
        "    # TODO: Use backbone features to predict conf_scores, offsets, and         #\n",
        "    # class_scores. Make sure conf_scores is between 0 and 1 by squashing the  #\n",
        "    # network output with a sigmoid. Also make sure the first two elements t^x #\n",
        "    # and t^y of offsets are between -0.5 and 0.5 by squashing with a sigmoid  #\n",
        "    # and subtracting 0.5.                                                     #\n",
        "    #                                                                          #\n",
        "    # During training you need to extract the outputs for only the positive    #\n",
        "    # and negative anchors as specified above.                                 #\n",
        "    #                                                                          #\n",
        "    # HINT: You can use the provided helper methods self._extract_anchor_data  #\n",
        "    # and self._extract_class_scores to extract information for positive and   #\n",
        "    # negative anchors specified by pos_anchor_idx and neg_anchor_idx.         #\n",
        "    ############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return conf_scores, offsets, class_scores"
      ],
      "metadata": {
        "id": "mHzyheeinJ8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah05Gd6BOKG2"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "Loss function은 앞에서 서술했듯이 3개의 부분으로 구성되어 있습니다. 각 부분이 어떤 의미를 갖는지 아래 코드를 읽어보세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDwpyHZBxNRn"
      },
      "source": [
        "#### Confidence score regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmVyv6NrxTiM"
      },
      "outputs": [],
      "source": [
        "def ConfScoreRegression(conf_scores, GT_conf_scores):\n",
        "  \"\"\"\n",
        "  Use sum-squared error as in YOLO\n",
        "\n",
        "  Inputs:\n",
        "  - conf_scores: Predicted confidence scores\n",
        "  - GT_conf_scores: GT confidence scores\n",
        "\n",
        "  Outputs:\n",
        "  - conf_score_loss\n",
        "  \"\"\"\n",
        "  # the target conf_scores for negative samples are zeros\n",
        "  GT_conf_scores = torch.cat((torch.ones_like(GT_conf_scores), \\\n",
        "                              torch.zeros_like(GT_conf_scores)), dim=0).view(-1, 1)\n",
        "  conf_score_loss = torch.sum((conf_scores - GT_conf_scores)**2) * 1. / GT_conf_scores.shape[0]\n",
        "  return conf_score_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRyF6HDGxT7P"
      },
      "source": [
        "#### Bounding box regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yecLoQLjxcx7"
      },
      "outputs": [],
      "source": [
        "def BboxRegression(offsets, GT_offsets):\n",
        "  \"\"\"\"\n",
        "  Use sum-squared error as in YOLO\n",
        "  For both xy and wh\n",
        "\n",
        "  Inputs:\n",
        "  - offsets: Predicted box offsets\n",
        "  - GT_offsets: GT box offsets\n",
        "\n",
        "  Outputs:\n",
        "  - bbox_reg_loss\n",
        "  \"\"\"\n",
        "  bbox_reg_loss = torch.sum((offsets - GT_offsets)**2) * 1. / GT_offsets.shape[0]\n",
        "  return bbox_reg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lADrqUuoxdRb"
      },
      "source": [
        "#### Object classifiction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FoLOeypxpC8"
      },
      "outputs": [],
      "source": [
        "def ObjectClassification(class_prob, GT_class, batch_size, anc_per_img, activated_anc_ind):\n",
        "  \"\"\"\"\n",
        "  Use softmax loss\n",
        "\n",
        "  Inputs:\n",
        "  - class_prob: Predicted softmax class probability\n",
        "  - GT_class: GT box class label\n",
        "\n",
        "  Outputs:\n",
        "  - object_cls_loss\n",
        "  \"\"\"\n",
        "  # average within sample and then average across batch\n",
        "  # such that the class pred would not bias towards dense popular objects like `person`\n",
        "  all_loss = F.cross_entropy(class_prob, GT_class, reduction='none') # , reduction='sum') * 1. / batch_size\n",
        "  object_cls_loss = 0\n",
        "  for idx in range(batch_size):\n",
        "    anc_ind_in_img = (activated_anc_ind >= idx * anc_per_img) & (activated_anc_ind < (idx+1) * anc_per_img)\n",
        "    object_cls_loss += all_loss[anc_ind_in_img].sum() * 1. / torch.sum(anc_ind_in_img)\n",
        "  object_cls_loss /= batch_size\n",
        "  # object_cls_loss = F.cross_entropy(class_prob, GT_class, reduction='sum') * 1. / batch_size\n",
        "\n",
        "  return object_cls_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJQGhhMTVi3k"
      },
      "source": [
        "Run the following to check your implementation. You should see errors on the order of 1e-8 or less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIt5AxlAxwKz"
      },
      "source": [
        "## Train an object detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yCYzKIxx2qB"
      },
      "source": [
        "### Object detection module\n",
        "\n",
        "앞에서 구현한 모든 함수를 합쳐서 SingleStageDetector class를 정의해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OsS-KZex6uK"
      },
      "outputs": [],
      "source": [
        "class SingleStageDetector(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]]) # READ ONLY\n",
        "    self.feat_extractor = FeatureExtractor()\n",
        "    self.num_classes = 20\n",
        "    self.pred_network = PredictionNetwork(1280, num_anchors=self.anchor_list.shape[0], \\\n",
        "                                          num_classes=self.num_classes)\n",
        "  def forward(self):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def inference(self):\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Im5jy1QRaeV"
      },
      "source": [
        "detector의 `forward` function을 직접 구현해봅시다. 해당 함수는 훈련 과정에서 input image와 GT bounding box를 받아서 minibatch에 대한 total loss를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsBG9yqNRWhu"
      },
      "outputs": [],
      "source": [
        "  def detector_forward(self, images, bboxes):\n",
        "    \"\"\"\n",
        "    Training-time forward pass for the single-stage detector.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Input images, of shape (B, 3, 224, 224)\n",
        "    - bboxes: GT bounding boxes of shape (B, N, 5) (padded)\n",
        "\n",
        "    Outputs:\n",
        "    - total_loss: Torch scalar giving the total loss for the batch.\n",
        "    \"\"\"\n",
        "    # weights to multiple to each loss term\n",
        "    w_conf = 1 # for conf_scores\n",
        "    w_reg = 1 # for offsets\n",
        "    w_cls = 1 # for class_prob\n",
        "\n",
        "    total_loss = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of SingleStageDetector.                   #\n",
        "    # A few key steps are outlined as follows:                                   #\n",
        "    # i) Image feature extraction,                                               #\n",
        "    # ii) Grid and anchor generation,                                            #\n",
        "    # iii) Compute IoU between anchors and GT boxes and then determine activated/#\n",
        "    #      negative anchors, and GT_conf_scores, GT_offsets, GT_class,           #\n",
        "    # iv) Compute conf_scores, offsets, class_prob through the prediction network#\n",
        "    # v) Compute the total_loss which is formulated as:                          #\n",
        "    #    total_loss = w_conf * conf_loss + w_reg * reg_loss + w_cls * cls_loss,  #\n",
        "    #    where conf_loss is determined by ConfScoreRegression, w_reg by          #\n",
        "    #    BboxRegression, and w_cls by ObjectClassification.                      #\n",
        "    # HINT: Set `neg_thresh=0.2` in ReferenceOnActivatedAnchors in this notebook #\n",
        "    #       (A5-1) for a better performance than with the default value.         #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    return total_loss\n",
        "\n",
        "  SingleStageDetector.forward = detector_forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXZAaDklx7Bs"
      },
      "source": [
        "### Object detection solver\n",
        "`DetectionSolver` object는 training을 위한 loop를 작동시킵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8KX5ohHyBFA"
      },
      "outputs": [],
      "source": [
        "def DetectionSolver(detector, train_loader, learning_rate=3e-3,\n",
        "                    lr_decay=1, num_epochs=20, **kwargs):\n",
        "  \"\"\"\n",
        "  Run optimization to train the model.\n",
        "  \"\"\"\n",
        "\n",
        "  # ship model to GPU\n",
        "  detector.to(**to_float_cuda)\n",
        "\n",
        "  # optimizer setup\n",
        "  from torch import optim\n",
        "  # optimizer = optim.Adam(\n",
        "  optimizer = optim.SGD(\n",
        "    filter(lambda p: p.requires_grad, detector.parameters()),\n",
        "    learning_rate) # leave betas and eps by default\n",
        "  lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
        "                                             lambda epoch: lr_decay ** epoch)\n",
        "\n",
        "  # sample minibatch data\n",
        "  loss_history = []\n",
        "  detector.train()\n",
        "  for i in range(num_epochs):\n",
        "    start_t = time.time()\n",
        "    for iter_num, data_batch in enumerate(train_loader):\n",
        "      images, boxes, w_batch, h_batch, _ = data_batch\n",
        "      resized_boxes = coord_trans(boxes, w_batch, h_batch, mode='p2a')\n",
        "      images = images.to(**to_float_cuda)\n",
        "      resized_boxes = resized_boxes.to(**to_float_cuda)\n",
        "\n",
        "      loss = detector(images, resized_boxes)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      loss_history.append(loss.item())\n",
        "      optimizer.step()\n",
        "\n",
        "      print('(Iter {} / {})'.format(iter_num, len(train_loader)))\n",
        "\n",
        "    end_t = time.time()\n",
        "    print('(Epoch {} / {}) loss: {:.4f} time per epoch: {:.1f}s'.format(\n",
        "        i, num_epochs, loss.item(), end_t-start_t))\n",
        "\n",
        "    lr_scheduler.step()\n",
        "\n",
        "  # plot the training losses\n",
        "  plt.plot(loss_history)\n",
        "  plt.xlabel('Iteration')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.title('Training loss history')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-9nFPtLyDE_"
      },
      "source": [
        "### Overfit small data\n",
        "\n",
        "먼저 모델이 잘 작동하는지 확인해보기 위해 우리가 구현한 detector를 작은 data subset에 overfit하도록 만들어봅시다.\n",
        "\n",
        "잘 학습이 된다면 **200 epoch 학습 후 total loss가 0.3 부근 혹은 그보다 작은 값이 나와야 합니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNKLRL7HyHO0"
      },
      "outputs": [],
      "source": [
        "# monitor the training loss\n",
        "fix_random_seed(0)\n",
        "num_sample = 10\n",
        "small_dataset = torch.utils.data.Subset(train_dataset, torch.linspace(0, len(train_dataset)-1, steps=num_sample).long())\n",
        "small_train_loader = pascal_voc2007_loader(small_dataset, 10) # a new loader\n",
        "\n",
        "for lr in [1e-2]:\n",
        "  print('lr: ', lr)\n",
        "  detector = SingleStageDetector()\n",
        "  DetectionSolver(detector, small_train_loader, learning_rate=lr, num_epochs=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuSBfcGWyHlD"
      },
      "source": [
        "### Train a net\n",
        "\n",
        "이제 더 많은 데이터를 더 길게 학습시켜봅시다. 50 epoch 학습을 진행했을 때, 우리는 0.27 근방 혹은 그보다 낮은 loss 값을 가져야 합니다.\n",
        "\n",
        "제가 직접 실행시켜봤을 때 Colab A100 환경에서 거의 1 epoch에 5-6분 정도가 소모되기 때문에, 50 epoch 실행을 위해서는 약 5시간이 소모될 것 같습니다. 이것이 너무 부담되시는 분께서는 10 epoch 학습 기준 0.47~0.48 부근의 loss가 나오면 학습이 어느 정도 된다고 확인하시고, epoch 수를 자유롭게 조정하셔도 좋을 것 같습니다. 하지만 50 epoch full training을 하는 것을 권장드립니다! :)\n",
        "\n",
        "(실제로는 object detection system은 12-24시간을 여러 개의 엄청나게 빠른 GPU들을 사용해서 학습시킵니다. 우리의 결과는 당연히 SOTA에 가깝지는 못하겠지만, 충분히 좋은 결과를 얻을 수는 있습니다.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aipf7-XQyJ28"
      },
      "outputs": [],
      "source": [
        "# monitor the training loss\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 100) # a new loader\n",
        "\n",
        "lr = 5e-2\n",
        "num_epochs = 50\n",
        "yolo_detector = SingleStageDetector()\n",
        "DetectionSolver(yolo_detector, train_loader, learning_rate=lr, num_epochs=num_epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_K7nL8eviXV"
      },
      "outputs": [],
      "source": [
        "# (optional) load/save checkpoint\n",
        "# torch.save(yolo_detector.state_dict(), 'yolo_detector.pt') # uncomment to save your checkpoint\n",
        "# yolo_detector.load_state_dict(torch.load('yolo_detector.pt')) # uncomment to load your previous checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzU71hc_y9Ij"
      },
      "source": [
        "## Use an object detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxwNNfV-zLkJ"
      },
      "source": [
        "### Thresholding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e42TAEcpjeKW"
      },
      "source": [
        "### Non-Maximum Suppression (NMS)\n",
        "Non-Max Suppression도 다들 기억하시죠? 직접 구현해봅시다. 기억 안 나시는 분들은 다음 자료를 참고해주세요. (p40-44): https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture15.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeWgWrzfYgm_"
      },
      "outputs": [],
      "source": [
        "def nms(boxes, scores, iou_threshold=0.5, topk=None):\n",
        "  \"\"\"\n",
        "  Non-maximum suppression removes overlapping bounding boxes.\n",
        "\n",
        "  Inputs:\n",
        "  - boxes: top-left and bottom-right coordinate values of the bounding boxes\n",
        "    to perform NMS on, of shape Nx4\n",
        "  - scores: scores for each one of the boxes, of shape N\n",
        "  - iou_threshold: discards all overlapping boxes with IoU > iou_threshold; float\n",
        "  - topk: If this is not None, then return only the topk highest-scoring boxes.\n",
        "    Otherwise if this is None, then return all boxes that pass NMS.\n",
        "\n",
        "  Outputs:\n",
        "  - keep: torch.long tensor with the indices of the elements that have been\n",
        "    kept by NMS, sorted in decreasing order of scores; of shape [num_kept_boxes]\n",
        "  \"\"\"\n",
        "\n",
        "  if (not boxes.numel()) or (not scores.numel()):\n",
        "    return torch.zeros(0, dtype=torch.long)\n",
        "\n",
        "  keep = None\n",
        "  #############################################################################\n",
        "  # TODO: Implement non-maximum suppression which iterates the following:     #\n",
        "  #       1. Select the highest-scoring box among the remaining ones,         #\n",
        "  #          which has not been chosen in this step before                    #\n",
        "  #       2. Eliminate boxes with IoU > threshold                             #\n",
        "  #       3. If any boxes remain, GOTO 1                                      #\n",
        "  #       Your implementation should not depend on a specific device type;    #\n",
        "  #       you can use the device of the input if necessary.                   #\n",
        "  # HINT: You can refer to the torchvision library code:                      #\n",
        "  #   github.com/pytorch/vision/blob/master/torchvision/csrc/cpu/nms_cpu.cpp  #\n",
        "  #############################################################################\n",
        "  # Replace \"pass\" statement with your code\n",
        "  pass\n",
        "  #############################################################################\n",
        "  #                              END OF YOUR CODE                             #\n",
        "  #############################################################################\n",
        "  return keep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq1biRRs6Rqf"
      },
      "source": [
        "아래 코드를 통해 여러분이 직접 구현한 NMS와 torchvision 상에 구현된 NMS를 비교할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqXkUdvdHh-U"
      },
      "outputs": [],
      "source": [
        "fix_random_seed(0)\n",
        "boxes = (100. * torch.rand(5000, 4)).round()\n",
        "boxes[:,2] = boxes[:,2] + boxes[:,0] + 1.\n",
        "boxes[:,3] = boxes[:,3] + boxes[:,1] + 1.\n",
        "scores = torch.randn(5000)\n",
        "\n",
        "names = ['your_cpu', 'torchvision_cpu', 'torchvision_cuda']\n",
        "iou_thresholds = [0.3, 0.5, 0.7]\n",
        "elapsed = dict(zip(names, [0.]*len(names)))\n",
        "intersects = dict(zip(names[1:], [0.]*(len(names)-1)))\n",
        "\n",
        "for iou_threshold in iou_thresholds:\n",
        "  tic = time.time()\n",
        "  my_keep = nms(boxes, scores, iou_threshold)\n",
        "  elapsed['your_cpu'] += time.time() - tic\n",
        "\n",
        "  tic = time.time()\n",
        "  tv_keep = torchvision.ops.nms(boxes, scores, iou_threshold)\n",
        "\n",
        "  elapsed['torchvision_cpu'] += time.time() - tic\n",
        "  intersect = len(set(tv_keep.tolist()).intersection(my_keep.tolist()))\n",
        "  intersects['torchvision_cpu'] += intersect / (len(my_keep) + len(tv_keep) - intersect)\n",
        "\n",
        "  tic = time.time()\n",
        "  tv_cuda_keep = torchvision.ops.nms(boxes.cuda(), scores.cuda(), iou_threshold).to(my_keep.device)\n",
        "  torch.cuda.synchronize()\n",
        "  elapsed['torchvision_cuda'] += time.time() - tic\n",
        "  intersect = len(set(tv_cuda_keep.tolist()).intersection(my_keep.tolist()))\n",
        "  intersects['torchvision_cuda'] += intersect / (len(my_keep) + len(tv_cuda_keep) - intersect)\n",
        "\n",
        "for key in intersects:\n",
        "  intersects[key] /= len(iou_thresholds)\n",
        "\n",
        "# You should see < 1% difference\n",
        "print('Testing NMS:')\n",
        "print('Your        CPU  implementation: %fs' % elapsed['your_cpu'])\n",
        "print('torchvision CPU  implementation: %fs' % elapsed['torchvision_cpu'])\n",
        "print('torchvision CUDA implementation: %fs' % elapsed['torchvision_cuda'])\n",
        "print('Speedup CPU : %fx' % (elapsed['your_cpu'] / elapsed['torchvision_cpu']))\n",
        "print('Speedup CUDA: %fx' % (elapsed['your_cpu'] / elapsed['torchvision_cuda']))\n",
        "print('Difference CPU : ', 1. - intersects['torchvision_cpu']) # in the order of 1e-3 or less\n",
        "print('Difference CUDA: ', 1. - intersects['torchvision_cuda']) # in the order of 1e-3 or less"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JSTPMsqzEnr"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d97pmEHLSDyK"
      },
      "source": [
        "이제 `SingleStageDetector`의 inference 부분을 구현해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqc-YGmOSG2Y"
      },
      "outputs": [],
      "source": [
        "  def detector_inference(self, images, thresh=0.5, nms_thresh=0.7):\n",
        "    \"\"\"\"\n",
        "    Inference-time forward pass for the single stage detector.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Input images\n",
        "    - thresh: Threshold value on confidence scores\n",
        "    - nms_thresh: Threshold value on NMS\n",
        "\n",
        "    Outputs:\n",
        "    - final_propsals: Keeped proposals after confidence score thresholding and NMS,\n",
        "                      a list of B (*x4) tensors\n",
        "    - final_conf_scores: Corresponding confidence scores, a list of B (*x1) tensors\n",
        "    - final_class: Corresponding class predictions, a list of B  (*x1) tensors\n",
        "    \"\"\"\n",
        "    final_proposals, final_conf_scores, final_class = [], [], []\n",
        "    ##############################################################################\n",
        "    # TODO: Predicting the final proposal coordinates `final_proposals`,         #\n",
        "    # confidence scores `final_conf_scores`, and the class index `final_class`.  #\n",
        "    # The overall steps are similar to the forward pass but now you do not need  #\n",
        "    # to decide the activated nor negative anchors.                              #\n",
        "    # HINT: Thresholding the conf_scores based on the threshold value `thresh`.  #\n",
        "    # Then, apply NMS (torchvision.ops.nms) to the filtered proposals given the  #\n",
        "    # threshold `nms_thresh`.                                                    #\n",
        "    # The class index is determined by the class with the maximal probability.   #\n",
        "    # Note that `final_propsals`, `final_conf_scores`, and `final_class` are all #\n",
        "    # lists of B 2-D tensors (you may need to unsqueeze dim=1 for the last two). #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return final_proposals, final_conf_scores, final_class\n",
        "\n",
        "  SingleStageDetector.inference = detector_inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6abC15U1Wtu"
      },
      "outputs": [],
      "source": [
        "def DetectionInference(detector, data_loader, dataset, idx_to_class, thresh=0.8, nms_thresh=0.3, output_dir=None):\n",
        "\n",
        "  detector.to(**to_float_cuda)\n",
        "\n",
        "  detector.eval()\n",
        "  start_t = time.time()\n",
        "\n",
        "  if output_dir is not None:\n",
        "    det_dir = 'mAP/input/detection-results'\n",
        "    gt_dir = 'mAP/input/ground-truth'\n",
        "    if os.path.exists(det_dir):\n",
        "      shutil.rmtree(det_dir)\n",
        "    os.mkdir(det_dir)\n",
        "    if os.path.exists(gt_dir):\n",
        "      shutil.rmtree(gt_dir)\n",
        "    os.mkdir(gt_dir)\n",
        "\n",
        "  for iter_num, data_batch in enumerate(data_loader):\n",
        "    images, boxes, w_batch, h_batch, img_ids = data_batch\n",
        "    images = images.to(**to_float_cuda)\n",
        "\n",
        "    final_proposals, final_conf_scores, final_class = detector.inference(images, thresh=thresh, nms_thresh=nms_thresh)\n",
        "\n",
        "    # clamp on the proposal coordinates\n",
        "    batch_size = len(images)\n",
        "    for idx in range(batch_size):\n",
        "      torch.clamp_(final_proposals[idx][:, 0::2], min=0, max=w_batch[idx])\n",
        "      torch.clamp_(final_proposals[idx][:, 1::2], min=0, max=h_batch[idx])\n",
        "\n",
        "      # visualization\n",
        "      # get the original image\n",
        "      # hack to get the original image so we don't have to load from local again...\n",
        "      i = batch_size*iter_num + idx\n",
        "      img, _ = dataset.__getitem__(i)\n",
        "\n",
        "      valid_box = sum([1 if j != -1 else 0 for j in boxes[idx][:, 0]])\n",
        "      final_all = torch.cat((final_proposals[idx], \\\n",
        "        final_class[idx].float(), final_conf_scores[idx]), dim=-1).cpu()\n",
        "      with torch.no_grad():\n",
        "        resized_proposals = coord_trans(final_all, w_batch[idx], h_batch[idx])\n",
        "\n",
        "      # write results to file for evaluation (use mAP API https://github.com/Cartucho/mAP for now...)\n",
        "      if output_dir is not None:\n",
        "        file_name = img_ids[idx].replace('.jpg', '.txt')\n",
        "        with open(os.path.join(det_dir, file_name), 'w') as f_det, \\\n",
        "          open(os.path.join(gt_dir, file_name), 'w') as f_gt:\n",
        "          print('{}: {} GT bboxes and {} proposals'.format(img_ids[idx], valid_box, resized_proposals.shape[0]))\n",
        "          for b in boxes[idx][:valid_box]:\n",
        "            f_gt.write('{} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[0], b[1], b[2], b[3]))\n",
        "          for b in resized_proposals:\n",
        "            f_det.write('{} {:.6f} {:.2f} {:.2f} {:.2f} {:.2f}\\n'.format(idx_to_class[b[4].item()], b[5], b[0], b[1], b[2], b[3]))\n",
        "      else:\n",
        "        data_visualizer(img, idx_to_class, boxes[idx][:valid_box], resized_proposals)\n",
        "\n",
        "  end_t = time.time()\n",
        "  print('Total inference time: {:.1f}s'.format(end_t-start_t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG4PPW1XEDTm"
      },
      "source": [
        "#### Inference - overfit small data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gp_Hmt-Km5bl"
      },
      "outputs": [],
      "source": [
        "# visualize the output from the overfitted model on small dataset\n",
        "# the bounding boxes should be really accurate\n",
        "DetectionInference(detector, small_train_loader, small_dataset, idx_to_class, thresh=0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifdEPmd9EMCP"
      },
      "source": [
        "#### Inference - train a net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7ArGiLTnHta"
      },
      "outputs": [],
      "source": [
        "# visualize the same output from the model trained on the entire training set\n",
        "# some bounding boxes might not make sense\n",
        "DetectionInference(yolo_detector, small_train_loader, small_dataset, idx_to_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETU6ev7aydIY"
      },
      "source": [
        "### Evaluation\n",
        "이제 모델 평가를 위해 mean Average Precision (mAP)를 계산해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fGptrealquF"
      },
      "source": [
        "아래 코드를 PASCAL VOC validation set에 대해 모델을 평가하기 위해 실행시켰을 때, 구현이 올바르게 되었다면 여러분은 11% 이상의 mAP를 얻어야 합니다.\n",
        "\n",
        "해당 dataset에 대한 SOTA model들은 80%가 넘는 mAP 값을 가집니다!\n",
        "이를 달성하기 위해서는 우리가 과제에서 구현한 것보다는 더 큰 네트워크로 더 많은 데이터를 더 길게 학습시켜야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvDb7uwqyhAK"
      },
      "outputs": [],
      "source": [
        "DetectionInference(yolo_detector, val_loader, val_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3)\n",
        "# DetectionInference(yolo_detector, train_loader, train_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3) # uncomment to see training mAP\n",
        "!cd mAP && python main.py --no-plot"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Two-stage Object Detector; Faster R-CNN\n",
        "\n",
        "모두 One-stage Object Detector 구현을 무사히 마치셨다면 후반부는 Two-stage Detector를 구현합니다!"
      ],
      "metadata": {
        "id": "JHLWHxy7t3TC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRc7P-RvRZGZ"
      },
      "source": [
        "## Region Proposal Networks (RPN)\n",
        "\n",
        "Faster R-CNN의 첫 번째 stage는 *Region Proposal Network (RPN)*입니다. RPN은 anchor들이 object를 포함하는지 혹은 그렇지 않은지 분류하고, anchor box로부터 region proposal로의 regression을 진행합니다.\n",
        "\n",
        "RPN은 사실 위에서 구현했던 single-stage detector와 매우 유사하고, 차이는 RPN에서는 classification score를 예측하지 않는다는 것입니다. 따라서 우리는 위에서 구현한 다양한 함수를 RPN을 구현하는 과정에서도 사용할 것입니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zik1hzyuILT"
      },
      "source": [
        "### Anchor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7XqqUyOuILT"
      },
      "source": [
        "# Declare variables for anchor priors, a Ax2 Tensor where A is the number of anchors.\n",
        "# Hand-picked, same as our two-stage detector.\n",
        "anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]], **to_float_cuda)\n",
        "print(anchor_list.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iHgTW0KTj13"
      },
      "source": [
        "### Activated (positive) and negative anchors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oalEB2-Pa1zQ"
      },
      "source": [
        "Anchor generation과 ground-truth matching은  `GenerateGrid`, `GenerateAnchor`, `IoU`, and `ReferenceOnActivatedAnchors` 함수를 재사용하여 구현합니다.\n",
        "\n",
        "error는 1e-7 단위 혹은 그보다 작은 값이 나와야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56qFFktVuILU"
      },
      "source": [
        "fix_random_seed(0)\n",
        "\n",
        "grid_list = GenerateGrid(w_list.shape[0])\n",
        "anc_list = GenerateAnchor(anchor_list, grid_list)\n",
        "iou_mat = IoU(anc_list, resized_box_list)\n",
        "activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, \\\n",
        "  activated_anc_coord, negative_anc_coord = ReferenceOnActivatedAnchors(anc_list, resized_box_list, grid_list, iou_mat)\n",
        "\n",
        "expected_GT_conf_scores = torch.tensor([0.74538743, 0.72793430, 0.71128041, 0.70029843,\n",
        "                                        0.75670898, 0.76044953, 0.37116671, 0.37116671], **to_float_cuda)\n",
        "expected_GT_offsets = torch.tensor([[ 0.01633334,  0.11911901, -0.09431065,  0.19244696],\n",
        "                                    [-0.03675002,  0.09324861, -0.00250307,  0.25213102],\n",
        "                                    [-0.03675002, -0.15675139, -0.00250307,  0.25213102],\n",
        "                                    [-0.02940002,  0.07459889, -0.22564663,  0.02898745],\n",
        "                                    [ 0.11879997,  0.03208542,  0.20863886, -0.07974572],\n",
        "                                    [-0.08120003,  0.03208542,  0.20863886, -0.07974572],\n",
        "                                    [ 0.07699990,  0.28533328, -0.03459148, -0.86750042],\n",
        "                                    [ 0.07699990, -0.21466672, -0.03459148, -0.86750042]], **to_float_cuda)\n",
        "expected_GT_class = torch.tensor([ 6,  7,  7,  7, 19, 19,  6,  6], **to_long_cuda)\n",
        "print('conf scores error: ', rel_error(GT_conf_scores, expected_GT_conf_scores))\n",
        "print('offsets error: ', rel_error(GT_offsets, expected_GT_offsets))\n",
        "print('class prob error: ', rel_error(GT_class, expected_GT_class))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPvX4TrgaLD8"
      },
      "source": [
        "# visualize the activated anchors\n",
        "anc_per_img = torch.prod(torch.tensor(anc_list.shape[1:-1]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Activated (positive) anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (activated_anc_ind >= idx * anc_per_img) & (activated_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} activated anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(activated_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))\n",
        "\n",
        "print('*'*80)\n",
        "print('Negative anchors:')\n",
        "for img, bbox, idx in zip(img_list, box_list, torch.arange(box_list.shape[0])):\n",
        "  anc_ind_in_img = (negative_anc_ind >= idx * anc_per_img) & (negative_anc_ind < (idx+1) * anc_per_img)\n",
        "  print('{} negative anchors!'.format(torch.sum(anc_ind_in_img)))\n",
        "  data_visualizer(img, idx_to_class, bbox[:, :4], coord_trans(negative_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhsI7oxHuILU"
      },
      "source": [
        "### Proposal module\n",
        "위에서의 Prediction Network와 유사하지만 RPN은 object proposal score와 bounding box offset을 예측하기만 하면 됩니다. 이는 class-agnostic합니다.\n",
        "\n",
        "![pred_scores2](https://miro.medium.com/max/918/1*wB3ctS9WGNmw6pP_kjLjgg.png)\n",
        "\n",
        "여기서 $k$는 $A$와 같다고 생각하면 됩니다.\n",
        "\n",
        "Image credit: Ren et al, \"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\", NeurIPS 2015, https://arxiv.org/abs/1506.01497"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMkvupmCdnYH"
      },
      "source": [
        "class ProposalModule(nn.Module):\n",
        "  def __init__(self, in_dim, hidden_dim=256, num_anchors=9, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_anchors != 0)\n",
        "    self.num_anchors = num_anchors\n",
        "    ##############################################################################\n",
        "    # TODO: Define the region proposal layer - a sequential module with a 3x3    #\n",
        "    # conv layer, followed by a Dropout (p=drop_ratio), a Leaky ReLU and         #\n",
        "    # a 1x1 conv.                                                                #\n",
        "    # HINT: The output should be of shape Bx(Ax6)x7x7, where A=self.num_anchors. #\n",
        "    #       Determine the padding of the 3x3 conv layer given the output dim.    #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def _extract_anchor_data(self, anchor_data, anchor_idx):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    - anchor_data: Tensor of shape (B, A, D, H, W) giving a vector of length\n",
        "      D for each of A anchors at each point in an H x W grid.\n",
        "    - anchor_idx: int64 Tensor of shape (M,) giving anchor indices to extract\n",
        "\n",
        "    Returns:\n",
        "    - extracted_anchors: Tensor of shape (M, D) giving anchor data for each\n",
        "      of the anchors specified by anchor_idx.\n",
        "    \"\"\"\n",
        "    B, A, D, H, W = anchor_data.shape\n",
        "    anchor_data = anchor_data.permute(0, 1, 3, 4, 2).contiguous().view(-1, D)\n",
        "    extracted_anchors = anchor_data[anchor_idx]\n",
        "    return extracted_anchors\n",
        "\n",
        "  def forward(self, features, pos_anchor_coord=None, \\\n",
        "              pos_anchor_idx=None, neg_anchor_idx=None):\n",
        "    \"\"\"\n",
        "    Run the forward pass of the proposal module.\n",
        "\n",
        "    Inputs:\n",
        "    - features: Tensor of shape (B, in_dim, H', W') giving features from the\n",
        "      backbone network.\n",
        "    - pos_anchor_coord: Tensor of shape (M, 4) giving the coordinates of\n",
        "      positive anchors. Anchors are specified as (x_tl, y_tl, x_br, y_br) with\n",
        "      the coordinates of the top-left corner (x_tl, y_tl) and bottom-right\n",
        "      corner (x_br, y_br). During inference this is None.\n",
        "    - pos_anchor_idx: int64 Tensor of shape (M,) giving the indices of positive\n",
        "      anchors. During inference this is None.\n",
        "    - neg_anchor_idx: int64 Tensor of shape (M,) giving the indicdes of negative\n",
        "      anchors. During inference this is None.\n",
        "\n",
        "    The outputs from this module are different during training and inference.\n",
        "\n",
        "    During training, pos_anchor_coord, pos_anchor_idx, and neg_anchor_idx are\n",
        "    all provided, and we only output predictions for the positive and negative\n",
        "    anchors. During inference, these are all None and we must output predictions\n",
        "    for all anchors.\n",
        "\n",
        "    Outputs (during training):\n",
        "    - conf_scores: Tensor of shape (2M, 2) giving the classification scores\n",
        "      (object vs background) for each of the M positive and M negative anchors.\n",
        "    - offsets: Tensor of shape (M, 4) giving predicted transforms for the\n",
        "      M positive anchors.\n",
        "    - proposals: Tensor of shape (M, 4) giving predicted region proposals for\n",
        "      the M positive anchors.\n",
        "\n",
        "    Outputs (during inference):\n",
        "    - conf_scores: Tensor of shape (B, A, 2, H', W') giving the predicted\n",
        "      classification scores (object vs background) for all anchors\n",
        "    - offsets: Tensor of shape (B, A, 4, H', W') giving the predicted transforms\n",
        "      for all anchors\n",
        "    \"\"\"\n",
        "    if pos_anchor_coord is None or pos_anchor_idx is None or neg_anchor_idx is None:\n",
        "      mode = 'eval'\n",
        "    else:\n",
        "      mode = 'train'\n",
        "    conf_scores, offsets, proposals = None, None, None\n",
        "    ############################################################################\n",
        "    # TODO: Predict classification scores (object vs background) and transforms#\n",
        "    # for all anchors. During inference, simply output predictions for all     #\n",
        "    # anchors. During training, extract the predictions for only the positive  #\n",
        "    # and negative anchors as described above, and also apply the transforms to#\n",
        "    # the positive anchors to compute the coordinates of the region proposals. #\n",
        "    #                                                                          #\n",
        "    # HINT: You can extract information about specific proposals using the     #\n",
        "    # provided helper function self._extract_anchor_data.                      #\n",
        "    # HINT: You can compute proposal coordinates using the GenerateProposal    #\n",
        "    # function from the previous notebook.                                     #\n",
        "    ############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    if mode == 'train':\n",
        "      return conf_scores, offsets, proposals\n",
        "    elif mode == 'eval':\n",
        "      return conf_scores, offsets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-6KpPF7bYJr"
      },
      "source": [
        "Run the following to check your implementation. You should see errors on the order of 1e-7 or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MX2JCaOf0768"
      },
      "source": [
        "# sanity check\n",
        "fix_random_seed(0)\n",
        "prop_module = ProposalModule(1280, drop_ratio=0).to(**to_float_cuda)\n",
        "features = torch.linspace(-10., 10., steps=3*1280*7*7, **to_float_cuda).view(3, 1280, 7, 7)\n",
        "conf_scores, offsets, proposals = prop_module(features, activated_anc_coord, \\\n",
        "              pos_anchor_idx=activated_anc_ind, neg_anchor_idx=negative_anc_ind)\n",
        "\n",
        "expected_conf_scores = torch.tensor([[-0.50843990,  2.62025023],\n",
        "                                     [-0.55775326, -0.29983672],\n",
        "                                     [-0.55796617, -0.30000290],\n",
        "                                     [ 0.17819080, -0.42211828],\n",
        "                                     [-0.51439995, -0.47708601],\n",
        "                                     [-0.51439744, -0.47703803],\n",
        "                                     [ 0.63225138,  2.71269488],\n",
        "                                     [ 0.63224381,  2.71290708]], **to_float_cuda)\n",
        "expected_offsets = torch.tensor([[ 1.62754285,  1.35253453, -1.85451591, -1.77882397],\n",
        "                                 [-0.33651856, -0.14402901, -0.07458937, -0.27201492],\n",
        "                                 [-0.33671042, -0.14398587, -0.07479107, -0.27199429],\n",
        "                                 [ 0.06847382,  0.21062726,  0.09334904, -0.02446130],\n",
        "                                 [ 0.16506940, -0.30296192,  0.29626080,  0.32173073],\n",
        "                                 [ 0.16507357, -0.30302414,  0.29625297,  0.32169008],\n",
        "                                 [ 1.59992146, -0.75236654,  1.66449440,  2.05138564],\n",
        "                                 [ 1.60008609, -0.75249159,  1.66474164,  2.05162382]], **to_float_cuda)\n",
        "\n",
        "print('conf scores error: ', rel_error(conf_scores[:8], expected_conf_scores))\n",
        "print('offsets error: ', rel_error(offsets, expected_offsets))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlO2IUCnt4zu"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "confidence score regression loss는 activated/negative anchor 둘 다에 대해서 계산하지만, bounding box regression loss는 activated anchor에 대해서만 계산합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75QCAjEqt4zs"
      },
      "source": [
        "#### Confidence score regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hguMsde4t4zj"
      },
      "source": [
        "def ConfScoreRegression(conf_scores, batch_size):\n",
        "  \"\"\"\n",
        "  Binary cross-entropy loss\n",
        "\n",
        "  Inputs:\n",
        "  - conf_scores: Predicted confidence scores, of shape (2M, 2). Assume that the\n",
        "    first M are positive samples, and the last M are negative samples.\n",
        "\n",
        "  Outputs:\n",
        "  - conf_score_loss: Torch scalar\n",
        "  \"\"\"\n",
        "  # the target conf_scores for positive samples are ones and negative are zeros\n",
        "  M = conf_scores.shape[0] // 2\n",
        "  GT_conf_scores = torch.zeros_like(conf_scores)\n",
        "  GT_conf_scores[:M, 0] = 1.\n",
        "  GT_conf_scores[M:, 1] = 1.\n",
        "\n",
        "  conf_score_loss = F.binary_cross_entropy_with_logits(conf_scores, GT_conf_scores, \\\n",
        "                                     reduction='sum') * 1. / batch_size\n",
        "  return conf_score_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuVgug42t4zh"
      },
      "source": [
        "#### Bounding box regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcOYeku8t4zY"
      },
      "source": [
        "def BboxRegression(offsets, GT_offsets, batch_size):\n",
        "  \"\"\"\"\n",
        "  Use SmoothL1 loss as in Faster R-CNN\n",
        "\n",
        "  Inputs:\n",
        "  - offsets: Predicted box offsets, of shape (M, 4)\n",
        "  - GT_offsets: GT box offsets, of shape (M, 4)\n",
        "\n",
        "  Outputs:\n",
        "  - bbox_reg_loss: Torch scalar\n",
        "  \"\"\"\n",
        "  bbox_reg_loss = F.smooth_l1_loss(offsets, GT_offsets, reduction='sum') * 1. / batch_size\n",
        "  return bbox_reg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vfXnAcUbnMw"
      },
      "source": [
        "Run the following to check your implementation. You should see errors on the order of 1e-7 or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eSleGX9yTeo"
      },
      "source": [
        "conf_loss = ConfScoreRegression(conf_scores, features.shape[0])\n",
        "reg_loss = BboxRegression(offsets, GT_offsets, features.shape[0])\n",
        "print('conf loss: {:.4f}, reg loss: {:.4f}'.format(conf_loss, reg_loss))\n",
        "\n",
        "loss_all = torch.tensor([conf_loss.data, reg_loss.data], **to_float_cuda)\n",
        "expected_loss = torch.tensor([8.55673981, 5.10593748], **to_float_cuda)\n",
        "\n",
        "print('loss error: ', rel_error(loss_all, expected_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiPfXUHPupDE"
      },
      "source": [
        "### RPN module\n",
        "Region Proposal Network을 구현해봅시다. class prediction이 없는 `SingleStageDetector`라고 생각하셔도 좋습니다!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrnEwmgluq2Y"
      },
      "source": [
        "class RPN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # READ ONLY\n",
        "    self.anchor_list = torch.tensor([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [2, 3], [3, 2], [3, 5], [5, 3]])\n",
        "    self.feat_extractor = FeatureExtractor()\n",
        "    self.prop_module = ProposalModule(1280, num_anchors=self.anchor_list.shape[0])\n",
        "\n",
        "  def forward(self, images, bboxes, output_mode='loss'):\n",
        "    \"\"\"\n",
        "    Training-time forward pass for the Region Proposal Network.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, 224, 224) giving input images\n",
        "    - bboxes: Tensor of ground-truth bounding boxes, returned from the DataLoader\n",
        "    - output_mode: One of 'loss' or 'all' that determines what is returned:\n",
        "      If output_mode is 'loss' then the output is:\n",
        "      - total_loss: Torch scalar giving the total RPN loss for the minibatch\n",
        "      If output_mode is 'all' then the output is:\n",
        "      - total_loss: Torch scalar giving the total RPN loss for the minibatch\n",
        "      - pos_conf_scores: Tensor of shape (M, 1) giving the object classification\n",
        "        scores (object vs background) for the positive anchors\n",
        "      - proposals: Tensor of shape (M, 4) giving the coordiantes of the region\n",
        "        proposals for the positive anchors\n",
        "      - features: Tensor of features computed from the backbone network\n",
        "      - GT_class: Tensor of shape (M,) giving the ground-truth category label\n",
        "        for the positive anchors.\n",
        "      - pos_anchor_idx: Tensor of shape (M,) giving indices of positive anchors\n",
        "      - neg_anchor_idx: Tensor of shape (M,) giving indices of negative anchors\n",
        "      - anc_per_image: Torch scalar giving the number of anchors per image.\n",
        "\n",
        "    Outputs: See output_mode\n",
        "\n",
        "    HINT: The function ReferenceOnActivatedAnchors from the previous notebook\n",
        "    can compute many of these outputs -- you should study it in detail:\n",
        "    - pos_anchor_idx (also called activated_anc_ind)\n",
        "    - neg_anchor_idx (also called negative_anc_ind)\n",
        "    - GT_class\n",
        "    \"\"\"\n",
        "    # weights to multiply to each loss term\n",
        "    w_conf = 1 # for conf_scores\n",
        "    w_reg = 5 # for offsets\n",
        "\n",
        "    assert output_mode in ('loss', 'all'), 'invalid output mode!'\n",
        "    total_loss = None\n",
        "    conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = \\\n",
        "      None, None, None, None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of RPN.                                   #\n",
        "    # A few key steps are outlined as follows:                                   #\n",
        "    # i) Image feature extraction,                                               #\n",
        "    # ii) Grid and anchor generation,                                            #\n",
        "    # iii) Compute IoU between anchors and GT boxes and then determine activated/#\n",
        "    #      negative anchors, and GT_conf_scores, GT_offsets, GT_class,           #\n",
        "    # iv) Compute conf_scores, offsets, proposals through the region proposal    #\n",
        "    #     module                                                                 #\n",
        "    # v) Compute the total_loss for RPN which is formulated as:                  #\n",
        "    #    total_loss = w_conf * conf_loss + w_reg * reg_loss,                     #\n",
        "    #    where conf_loss is determined by ConfScoreRegression, w_reg by          #\n",
        "    #    BboxRegression. Note that RPN does not predict any class info.          #\n",
        "    #    We have written this part for you which you've already practiced earlier#\n",
        "    # HINT: Do not apply thresholding nor NMS on the proposals during training   #\n",
        "    #       as positive/negative anchors have been explicitly targeted.          #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "    if output_mode == 'loss':\n",
        "      return total_loss\n",
        "    else:\n",
        "      return total_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img\n",
        "\n",
        "\n",
        "  def inference(self, images, thresh=0.5, nms_thresh=0.7, mode='RPN'):\n",
        "    \"\"\"\n",
        "    Inference-time forward pass for the Region Proposal Network.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, H, W) giving input images\n",
        "    - thresh: Threshold value on confidence scores. Proposals with a predicted\n",
        "      object probability above thresh should be kept. HINT: You can convert the\n",
        "      object score to an object probability using a sigmoid nonlinearity.\n",
        "    - nms_thresh: IoU threshold for non-maximum suppression\n",
        "    - mode: One of 'RPN' or 'FasterRCNN' to determine the outputs.\n",
        "\n",
        "    The region proposal network can output a variable number of region proposals\n",
        "    per input image. We assume that the input image images[i] gives rise to\n",
        "    P_i final propsals after thresholding and NMS.\n",
        "\n",
        "    NOTE: NMS is performed independently per-image!\n",
        "\n",
        "    Outputs:\n",
        "    - final_proposals: List of length B, where final_proposals[i] is a Tensor\n",
        "      of shape (P_i, 4) giving the coordinates of the predicted region proposals\n",
        "      for the input image images[i].\n",
        "    - final_conf_probs: List of length B, where final_conf_probs[i] is a\n",
        "      Tensor of shape (P_i,) giving the predicted object probabilities for each\n",
        "      predicted region proposal for images[i]. Note that these are\n",
        "      *probabilities*, not scores, so they should be between 0 and 1.\n",
        "    - features: Tensor of shape (B, D, H', W') giving the image features\n",
        "      predicted by the backbone network for each element of images.\n",
        "      If mode is \"RPN\" then this is a dummy list of zeros instead.\n",
        "    \"\"\"\n",
        "    assert mode in ('RPN', 'FasterRCNN'), 'invalid inference mode!'\n",
        "\n",
        "    features, final_conf_probs, final_proposals = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Predicting the RPN proposal coordinates `final_proposals` and        #\n",
        "    # confidence scores `final_conf_probs`.                                     #\n",
        "    # The overall steps are similar to the forward pass but now you do not need  #\n",
        "    # to decide the activated nor negative anchors.                              #\n",
        "    # HINT: Threshold the conf_scores based on the threshold value `thresh`.     #\n",
        "    # Then, apply NMS to the filtered proposals given the threshold `nms_thresh`.#\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    if mode == 'RPN':\n",
        "      features = [torch.zeros_like(i) for i in final_conf_probs] # dummy class\n",
        "    return final_proposals, final_conf_probs, features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmukyW51hRHa"
      },
      "source": [
        "### RPN solver\n",
        "Faster R-CNN에서는 RPN은 second-stage network와 jointly train하지만, 여기서는 RPN 구현을 test 해보기 위해 우리는 먼저 RPN만 학습시켜봅시다. 이는 기본적으로 class-agnostic single-stage detector입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6puWIsYQhQkW"
      },
      "source": [
        "RPNSolver = DetectionSolver # the same solver as in YOLO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVQPPDVchAGQ"
      },
      "source": [
        "### RPN - Overfit small data\n",
        "RPN을 먼저 small subset에 overfit시켜 봅시다. 훈련 이후 우리는 3.0 근방 혹은 그보다 낮은 loss 값을 얻어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTObddiog9wJ"
      },
      "source": [
        "# monitor the training loss\n",
        "num_sample = 10\n",
        "small_dataset = torch.utils.data.Subset(train_dataset, torch.linspace(0, len(train_dataset)-1, steps=num_sample).long())\n",
        "small_train_loader = pascal_voc2007_loader(small_dataset, 10) # a new loader\n",
        "\n",
        "for lr in [1e-3]:\n",
        "  print('lr: ', lr)\n",
        "  rpn = RPN()\n",
        "  RPNSolver(rpn, small_train_loader, learning_rate=lr, num_epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOIazbsrFEVc"
      },
      "source": [
        "### RPN - Inference\n",
        "`DetectionInference` function을 사용해 RPN 결과를 visualize해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2wxjsnYFDrw"
      },
      "source": [
        "RPNInference = DetectionInference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94vbKR2JJ-Mq"
      },
      "source": [
        "# visualize the output from the overfitted model on small dataset\n",
        "# the bounding boxes should be really accurate\n",
        "# ignore the dummy object class (in blue) as RPN does not output class!\n",
        "RPNInference(rpn, small_train_loader, small_dataset, idx_to_class, thresh=0.8, nms_thresh=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKjv6JLMRj7s"
      },
      "source": [
        "## Faster R-CNN\n",
        "우리는 Faster R-CNN의 '절반'을 현재까지 구현했다고 생각하시면 됩니다. 이제 나머지 절반을 구현해봅시다. 이 second half에 대한 설명은 다음과 같습니다. 아래 글을 직접 읽어보시고 구현에 들어가면 좋을 것 같습니다!\n",
        "\n",
        "*Given the proposals or region of interests (RoI) from RPN, we warp each region from CNN activation map to a fixed size 2x2 by using [RoI Align](https://arxiv.org/pdf/1703.06870.pdf). Essentially, the RoI feature is determined by bilinear interpolation over the CNN activation map. We meanpool the RoI feature over the spatial dimension (2x2).*\n",
        "\n",
        "*Finally, we classify the meanpooled RoI feature into class probabilities.*\n",
        "\n",
        "아래 글은 실제 full Faster R-CNN와 우리가 구현하는 two-stage detector의 차이를 설명하는 글입니다.   \n",
        "\n",
        "\n",
        "*For simplicity, our two-stage detector here differs from a full Faster R-CNN system in a few aspects.*\n",
        "\n",
        "*1. In a full implementation, the second stage of the network would predict a second set of offsets to transform the region proposal into a final predicted object bounding box. However we omit this for simplicity.*\n",
        "\n",
        "*2. In a full implementation, the second stage of the network should be able to reject negative boxes -- in other words, if we want to predict C different object categories then the final classification layer of the second stage would predict a distribution over C+1 categories, with an extra one for background. We omit this, as it requires extra bookeeping in the second stage about which proposals are positive / negative; so for simplicity our second stage will only predict a distribution over C categories, and we will assume that the RPN has filtered out all background regions.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdqlAcP6kRvA"
      },
      "source": [
        "### RoI Align\n",
        "\n",
        "RoIAlign 함수는 torchvision에 구현되어 있는 `roi_align` function을 사용하겠습니다. 자세한 내용은 다음 링크를 확인해보세요!\n",
        "\n",
        "https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umYbQz6Ct2bm"
      },
      "source": [
        "### Faster R-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmSpMc1zakpJ"
      },
      "source": [
        "class TwoStageDetector(nn.Module):\n",
        "  def __init__(self, in_dim=1280, hidden_dim=256, num_classes=20, \\\n",
        "               roi_output_w=2, roi_output_h=2, drop_ratio=0.3):\n",
        "    super().__init__()\n",
        "\n",
        "    assert(num_classes != 0)\n",
        "    self.num_classes = num_classes\n",
        "    self.roi_output_w, self.roi_output_h = roi_output_w, roi_output_h\n",
        "    ##############################################################################\n",
        "    # TODO: Declare your RPN and the region classification layer (in Fast R-CNN).#\n",
        "    # The region classification layer is a sequential module with a Linear layer,#\n",
        "    # followed by a Dropout (p=drop_ratio), a ReLU nonlinearity and another      #\n",
        "    # Linear layer that predicts classification scores for each proposal.        #\n",
        "    # HINT: The dimension of the two Linear layers are in_dim -> hidden_dim and  #\n",
        "    # hidden_dim -> num_classes.                                                 #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "\n",
        "  def forward(self, images, bboxes):\n",
        "    \"\"\"\n",
        "    Training-time forward pass for our two-stage Faster R-CNN detector.\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, H, W) giving input images\n",
        "    - bboxes: Tensor of shape (B, N, 5) giving ground-truth bounding boxes\n",
        "      and category labels, from the dataloader.\n",
        "\n",
        "    Outputs:\n",
        "    - total_loss: Torch scalar giving the overall training loss.\n",
        "    \"\"\"\n",
        "    total_loss = None\n",
        "    ##############################################################################\n",
        "    # TODO: Implement the forward pass of TwoStageDetector.                      #\n",
        "    # A few key steps are outlined as follows:                                   #\n",
        "    # i) RPN, including image feature extraction, grid/anchor/proposal           #\n",
        "    #       generation, activated and negative anchors determination.            #\n",
        "    # ii) Perform RoI Align on proposals and meanpool the feature in the spatial #\n",
        "    #     dimension.                                                             #\n",
        "    # iii) Pass the RoI feature through the region classification layer which    #\n",
        "    #      gives the class probilities.                                          #\n",
        "    # iv) Compute class_prob through the prediction network and compute the      #\n",
        "    #     cross entropy loss (cls_loss) between the prediction class_prob and    #\n",
        "    #      the reference GT_class. Hint: Use F.cross_entropy loss.               #\n",
        "    # v) Compute the total_loss which is formulated as:                          #\n",
        "    #    total_loss = rpn_loss + cls_loss.                                       #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return total_loss\n",
        "\n",
        "  def inference(self, images, thresh=0.5, nms_thresh=0.7):\n",
        "    \"\"\"\"\n",
        "    Inference-time forward pass for our two-stage Faster R-CNN detector\n",
        "\n",
        "    Inputs:\n",
        "    - images: Tensor of shape (B, 3, H, W) giving input images\n",
        "    - thresh: Threshold value on NMS object probabilities\n",
        "    - nms_thresh: IoU threshold for NMS in the RPN\n",
        "\n",
        "    We can output a variable number of predicted boxes per input image.\n",
        "    In particular we assume that the input images[i] gives rise to P_i final\n",
        "    predicted boxes.\n",
        "\n",
        "    Outputs:\n",
        "    - final_proposals: List of length (B,) where final_proposals[i] is a Tensor\n",
        "      of shape (P_i, 4) giving the coordinates of the final predicted boxes for\n",
        "      the input images[i]\n",
        "    - final_conf_probs: List of length (B,) where final_conf_probs[i] is a\n",
        "      Tensor of shape (P_i,) giving the predicted probabilites that the boxes\n",
        "      in final_proposals[i] are objects (vs background)\n",
        "    - final_class: List of length (B,), where final_class[i] is an int64 Tensor\n",
        "      of shape (P_i,) giving the predicted category labels for each box in\n",
        "      final_proposals[i].\n",
        "    \"\"\"\n",
        "    final_proposals, final_conf_probs, final_class = None, None, None\n",
        "    ##############################################################################\n",
        "    # TODO: Predicting the final proposal coordinates `final_proposals`,        #\n",
        "    # confidence scores `final_conf_probs`, and the class index `final_class`.  #\n",
        "    # The overall steps are similar to the forward pass but now you do not need #\n",
        "    # to decide the activated nor negative anchors.                             #\n",
        "    # HINT: Use the RPN inference function to perform thresholding and NMS, and #\n",
        "    # to compute final_proposals and final_conf_probs. Use the predicted class  #\n",
        "    # probabilities from the second-stage network to compute final_class.       #\n",
        "    ##############################################################################\n",
        "    # Replace \"pass\" statement with your code\n",
        "    pass\n",
        "    ##############################################################################\n",
        "    #                               END OF YOUR CODE                             #\n",
        "    ##############################################################################\n",
        "    return final_proposals, final_conf_probs, final_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFZ49wox4MYn"
      },
      "source": [
        "### Overfit small data\n",
        "\n",
        "Faster R-CNN을 먼저 small subset에 overfit시켜 봅시다. 훈련 이후 우리는 4.0 근방 혹은 그보다 낮은 loss 값을 얻어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbxeAJq0zc3F"
      },
      "source": [
        "# monitor the training loss\n",
        "\n",
        "lr = 1e-3\n",
        "detector = TwoStageDetector()\n",
        "DetectionSolver(detector, small_train_loader, learning_rate=lr, num_epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWA1DbG47ln"
      },
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJqyzeNHuILZ"
      },
      "source": [
        "# visualize the output from the overfitted model on small dataset\n",
        "# the bounding boxes should be really accurate\n",
        "DetectionInference(detector, small_train_loader, small_dataset, idx_to_class, thresh=0.8, nms_thresh=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr7wNngy4oZf"
      },
      "source": [
        "### Train a net\n",
        "\n",
        "이제 더 많은 데이터로 더 길게 학습시켜봅시다. 50 epoch 학습을 진행했을 때, 우리는 0.30 근방 혹은 그보다 낮은 loss 값을 가져야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1k1rx1f4sTE"
      },
      "source": [
        "# monitor the training loss\n",
        "train_loader = pascal_voc2007_loader(train_dataset, 100) # a new loader\n",
        "\n",
        "num_epochs = 50\n",
        "lr = 5e-3\n",
        "frcnn_detector = TwoStageDetector()\n",
        "DetectionSolver(frcnn_detector, train_loader, learning_rate=lr, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qERg85Ip7hwu"
      },
      "source": [
        "# (optional) load/save checkpoint\n",
        "# torch.save(frcnn_detector.state_dict(), 'frcnn_detector.pt') # uncomment to save your checkpoint\n",
        "# frcnn_detector.load_state_dict(torch.load('frcnn_detector.pt')) # uncomment to load your previous checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhWZT-ztEaqm"
      },
      "source": [
        "#### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TW698S51uILZ"
      },
      "source": [
        "# visualize the same output from the model trained on the entire training set\n",
        "# some bounding boxes might not make sense\n",
        "DetectionInference(frcnn_detector, small_train_loader, small_dataset, idx_to_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70y9S9WNuILa"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iGOlxhyvO3z"
      },
      "source": [
        "validation set에 대해 16% 이상의 mAP를 관찰할 수 있다면 여러분은 올바르게 구현을 한 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVjfVTAXuILa"
      },
      "source": [
        "!rm -r mAP/input/*\n",
        "DetectionInference(frcnn_detector, val_loader, val_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3)\n",
        "# DetectionInference(frcnn_detector, train_loader, train_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3) # uncomment to see training mAP\n",
        "!cd mAP && python main.py --no-plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Discussion"
      ],
      "metadata": {
        "id": "8YUjVO5Z1dF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "여러분, 과제하시느라 정말 고생 많으셨습니다!\n",
        "이 과제를 무사히 마쳤다면 여러분은 무려 2개의 논문을 거의 재구현해본 것과 같은 효과의 연습을 해보신 것입니다.\n",
        "물론 현재에 와서는 Faster R-CNN이나 초창기 YOLO 모델은 SOTA에서는 멀어진 모델들이지만, 충분히 구현해보고 공부해볼 가치가 있다고 생각합니다.\n",
        "\n",
        "마지막으로 과제와 관련된 Discussion 질문으로 과제를 마칠까 합니다!\n",
        "\n",
        "아래 텍스트 부분을 수정하여 여러분의 생각을 자유롭게 적어주시면 됩니다."
      ],
      "metadata": {
        "id": "420nCvZW1gSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) one-stage detector와 two-stage detector를 비교하고 장단점을 서술해주세요. 여러분이 과제를 하면서 느낀 점을 바탕으로 써주셔도 좋고, 추가적으로 자료를 찾아보시고 적어주셔도 좋습니다.\n",
        "\n",
        "**Answer:**\n"
      ],
      "metadata": {
        "id": "ByKz0EZ52CTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Faster R-CNN과 YOLO 둘 중에 하나를 선택해서, 각 detector를 개선시킬 수 있는 방법을 2가지 이상 제안해주세요.\n",
        "\n",
        "여기서 말하는 개선 방법이란 전반적인 네트워크 구조, 동일한 pipeline에서 특정 부분을 수정하는 것, backbone architecture 등 object detection을 한다는 틀 안에서 무엇이든 될 수 있습니다!\n",
        "\n",
        "여기서 반드시 개선시킬 수 있는 방법과 함께 **개선 방법에 대한 motivation이나 intuition**을 꼭 제시해주세요. 단순하게 이게 더 잘 될 거 같다라고 생각하는 것보다는 그 motivation을 확실하게 하고 잘 정리하는 것이 매우 중요하기 때문에, 꼭 연습해보시면 좋을 것 같습니다.\n",
        "\n",
        "**선택한 모델:**\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "yGVw6KcY2WuT"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}